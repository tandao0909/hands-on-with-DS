{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "housing_path = os.path.join(\"..\", \"datasets\", \"housing\", \"housing.csv\")\n",
    "housing_data = pd.read_csv(housing_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data[\"income_cat\"] = pd.cut(\n",
    "    housing_data[\"median_income\"],\n",
    "    bins=[0.0, 1.5, 3.0, 4.5, 6.0, np.inf],\n",
    "    labels=[1, 2, 3, 4, 5],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(\n",
    "    housing_data, test_size=0.2, random_state=42, stratify=housing_data[\"income_cat\"]\n",
    ")\n",
    "\n",
    "for set in (train_set, test_set):\n",
    "    set.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train: pd.DataFrame = train_set.drop(\"median_house_value\", axis=1)\n",
    "y_train: pd.DataFrame = train_set[\"median_house_value\"]\n",
    "\n",
    "X_test: pd.DataFrame = test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test: pd.DataFrame = test_set[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters, gamma, random_state) -> None:\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self  # always return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "\n",
    "    def get_feature_names_out(self, name=None):\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\n",
    "\n",
    "\n",
    "def column_ratio(X):\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "\n",
    "\n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "    return [\"ratio\"]  # features_name_out_ for ration pipeline\n",
    "\n",
    "\n",
    "def ratio_pipeline():\n",
    "    return make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        FunctionTransformer(column_ratio, feature_names_out=ratio_name),\n",
    "        StandardScaler(),\n",
    "    )\n",
    "\n",
    "\n",
    "cat_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"), OneHotEncoder(handle_unknown=\"ignore\")\n",
    ")\n",
    "\n",
    "log_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
    "    StandardScaler(),\n",
    ")\n",
    "\n",
    "cluster_similarity = ClusterSimilarity(n_clusters=10, gamma=1.0, random_state=42)\n",
    "\n",
    "default_num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    [\n",
    "        (\"bedrooms\", ratio_pipeline(), [\"total_bedrooms\", \"total_rooms\"]),\n",
    "        (\"rooms_per_house\", ratio_pipeline(), [\"total_rooms\", \"households\"]),\n",
    "        (\"people_per_house\", ratio_pipeline(), [\"population\", \"households\"]),\n",
    "        (\n",
    "            \"log\",\n",
    "            log_pipeline,\n",
    "            [\n",
    "                \"total_bedrooms\",\n",
    "                \"total_rooms\",\n",
    "                \"population\",\n",
    "                \"households\",\n",
    "                \"median_income\",\n",
    "            ],\n",
    "        ),\n",
    "        (\"geo\", cluster_similarity, [\"latitude\", \"longitude\"]),\n",
    "        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "    ],\n",
    "    remainder=default_num_pipeline,\n",
    ")  # one column remaining: housing_median_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than restrict ourselves to k-Nearest Neighbors regressors, let's create a transformer that accepts any regressor. For this, we can extend the `MetaEstimatorMixin` and have a required `estimator` argument in the constructor. The `fit()` method must work on a clone of this estimator, and it must also save `feature_names_in_`. The `MetaEstimatorMixin` will ensure that `estimator` is listed as a required parameters, and it will update `get_params()` and `set_params()` to make the estimator's hyperparameters available for tuning. Lastly, we create a `get_feature_names_out()` method: the output column name is the ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator, MetaEstimatorMixin, clone\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "class FeatureFromRegressor(BaseEstimator, TransformerMixin, MetaEstimatorMixin):\n",
    "    def __init__(self, estimator) -> None:\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        estimator_: BaseEstimator = clone(self.estimator)\n",
    "        estimator_.fit(X, y)\n",
    "        self.estimator_ = estimator_\n",
    "        self.n_features_in_ = self.estimator_.n_features_in_\n",
    "        if hasattr(self.estimator_, \"feature_names_in_\"):\n",
    "            self.feature_names_in_ = self.estimator_.feature_names_in_\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)\n",
    "        predictions = self.estimator_.predict(X)\n",
    "        # Because sklearn prefer input as an 2D array, not 1D, it is a good idea to reshape it to a 2D array.\n",
    "        if predictions.ndim == 1:\n",
    "            predictions = predictions.reshape(-1, 1)\n",
    "        return predictions\n",
    "\n",
    "    def get_feature_names_out_(self, names=None):\n",
    "        check_is_fitted(self)\n",
    "        n_outputs = getattr(self.estimator_, \"n_outputs_\", 1)\n",
    "        estimator_class_name = self.estimator_.__class__.__name__\n",
    "        estimator_short_name = estimator_class_name.lower().replace(\"_\", \"\")\n",
    "        return [f\"{estimator_short_name}_prediction_{i}\" for i in range(n_outputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if it compiles to Scikit-Learn API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "\n",
    "check_estimator(FeatureFromRegressor(KNeighborsRegressor()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 68850.],\n",
       "       [279600.],\n",
       "       [ 79000.],\n",
       "       ...,\n",
       "       [135700.],\n",
       "       [258100.],\n",
       "       [ 62700.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_reg = KNeighborsRegressor(n_neighbors=3, weights=\"distance\")\n",
    "knn_transformer = FeatureFromRegressor(knn_reg)\n",
    "geo_features = X_train[[\"latitude\", \"longitude\"]]\n",
    "knn_transformer.fit_transform(geo_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kneighborsregressor_prediction_0']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_transformer.get_feature_names_out_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = [\n",
    "    (name, clone(transformer), columns)\n",
    "    for name, transformer, columns in preprocessing.transformers\n",
    "]\n",
    "geo_index = [name for (name, _, _) in transformers].index(\"geo\")\n",
    "transformers[geo_index] = (\"geo\", knn_transformer, [\"latitude\", \"longitude\"])\n",
    "\n",
    "new_geo_preprocessing = ColumnTransformer(transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnd_search.best_params_ from housing notebook\n",
    "svr_C = 157055.10989448498\n",
    "svr_gamma = 0.26497040005002437\n",
    "svr_kernel = \"rbf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "new_geo_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessing\", new_geo_preprocessing),\n",
    "        (\"svr\", SVR(C=svr_C, gamma=svr_gamma, kernel=svr_kernel)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count         3.000000\n",
       "mean     104487.511247\n",
       "std        2940.605612\n",
       "min      101582.902572\n",
       "25%      102999.847802\n",
       "50%      104416.793032\n",
       "75%      105939.815585\n",
       "max      107462.838138\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "new_pipe_rmses = -cross_val_score(\n",
    "    new_geo_pipeline,\n",
    "    X_train.iloc[:5000],\n",
    "    y_train.iloc[:5000],\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=3,\n",
    ")\n",
    "pd.Series(new_pipe_rmses).describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handons-DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
