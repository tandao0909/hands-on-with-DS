{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 14:09:06.072594: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Embedded Reber grammars *were used by Hochreiter and Schmidhuber in [their paper](https://scholar.google.com/scholar?q=Long+Short-Term+Memory+author%3ASchmidhuber) about LSTMs. They are artificial grammars that produce strings such as “BPBTSXXVPSEPE”. Check out Jenny Orr’s [nice introduction](https://willamette.edu/~gorr/classes/cs449/reber.html) to this topic, then choose a particular embedded Reber grammar (such as the one represented on Orr’s page), then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don’t.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link to the Jenny Orr webpage is dead, but I found another [link](http://christianherta.de/lehre/dataScience/machineLearning/neuralNetworks/reberGrammar.php) that can be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reber_grammar = [\n",
    "    [(\"B\", 1)],  # (state 0) = B => (state 1)\n",
    "    [(\"T\", 2), (\"P\", 3)],  # (state 1) = T => (state 2) or = P => (state 3)\n",
    "    [(\"S\", 2), (\"X\", 4)],  # (state 2) = S => (state 2) or = X => (state 4)\n",
    "    [(\"T\", 3), (\"V\", 5)],  # and so on ...\n",
    "    [(\"X\", 3), (\"S\", 6)],\n",
    "    [(\"V\", 6)],\n",
    "    [(\"E\", None)],  # (state 6 ) = E => (terminal state)\n",
    "]\n",
    "\n",
    "embed_reber_grammar = [\n",
    "    [(\"B\", 1)],\n",
    "    [(\"T\", 2), (\"P\", 3)],\n",
    "    [(reber_grammar, 4)],\n",
    "    [(reber_grammar, 5)],\n",
    "    [(\"T\", 6)],\n",
    "    [(\"P\", 6)],\n",
    "    [(\"E\", None)],\n",
    "]\n",
    "\n",
    "\n",
    "def generate_string(grammar):\n",
    "    output = []\n",
    "    state = 0\n",
    "    while state is not None:\n",
    "        index = np.random.randint(len(grammar[state]))\n",
    "        production, state = grammar[state][index]\n",
    "        if isinstance(production, list):  # deal with reber grammar\n",
    "            production = generate_string(production)\n",
    "        output.append(production)\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a few string using the original Reber grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTXXTTVVE BTSSXXTTTVVE BTXSE BPTVVE BTXSE BPVVE BPVVE BPVVE BTSXSE BPTVVE BTSSSSXSE BPVVE BPTVVE BPTVVE BTXXVVE BPTTTTTTTTVVE BPTVVE BPVVE BPTVVE BTXSE BPTVVE BTXXVVE BTSXXVVE BPVVE BPVVE "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_string(reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. Let's look at the embed Reber grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTBPTTTVVETE BTBTSXXTTTVVETE BTBPVVETE BPBTXXVVEPE BPBPVVEPE BPBPVVEPE BPBTSXSEPE BPBTXXTTTTVVEPE BPBPVVEPE BPBTXSEPE BTBPTVVETE BTBPVVETE BTBTSSSSSSXSETE BTBPVVETE BPBPTVVEPE BTBPVVETE BPBTXXVVEPE BTBPTTVVETE BTBPVVETE BPBPVVEPE BPBPVVEPE BPBPVVEPE BTBTXSETE BPBPVVEPE BPBPVVEPE "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_string(embed_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to generate strings that do not respect grammar. We could create a random string, but the task would be quite easy, so we instead will generate a valid string, and corrupt it by changing only one letter at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSSIBLE_CHARS = \"BEPSTXV\"\n",
    "\n",
    "\n",
    "def generate_corrupted_string(grammar, chars=POSSIBLE_CHARS):\n",
    "    string = generate_string(grammar)\n",
    "    index = np.random.randint(len(string))\n",
    "    good_char = string[index]\n",
    "    bad_char = np.random.choice(sorted(set(chars) - set(good_char)))\n",
    "    return string[:index] + bad_char + string[index + 1 :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few corrupted strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTBPETTVVETE BTBTSSXPVVETE BPBTXEEPE BPBPTTVVEXE BTBPTTTTTVVTTE BPTTXSEPE BTBPVVETS BTBTSSSSSSETE BPBPVVEPP BTBPVVTTE TTBPTTVVETE BXBPVVEPE BPBXVVEPE BEBTXSETE BPBPVVEXE BTBPTVVVTE BPBTXXTVVBPE BPBPVVEPV BTBTSSSPXXVVETE BTBTXXETTTVVETE BBBPTTVVETE BPBTPXSEPE BPEPVVEPE BTBTSXVTTVVETE BTBTSSSXSETV "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_corrupted_string(embed_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We cannot feed string directly to an RNN, so we need to encode them somehow.\n",
    "- One option would be one-hot encode each character. Another options is to use embeddings.\n",
    "- We'll go with the second option, but since there are just a handful of characters, one-hot encoding can be used as well.\n",
    "- For embeddings to work, we need to convert each string into a sequence of character IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_ids(string, chars=POSSIBLE_CHARS):\n",
    "    return [chars.index(c) for c in string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 0, 2, 1, 4, 4, 6, 6, 1, 4, 1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_ids(\"BTBPETTVVETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(size):\n",
    "    good_strings = [\n",
    "        string_to_ids(generate_string(embed_reber_grammar)) for _ in range(size // 2)\n",
    "    ]\n",
    "    corrupted_strings = [\n",
    "        string_to_ids(generate_corrupted_string(embed_reber_grammar))\n",
    "        for _ in range(size - size // 2)\n",
    "    ]\n",
    "    all_strings = good_strings + corrupted_strings\n",
    "    X = tf.ragged.constant(all_strings, ragged_rank=1)\n",
    "    y = tf.constant(\n",
    "        [[1.0] for _ in range(len(good_strings))]\n",
    "        + [[0.0] for _ in range(len(corrupted_strings))]\n",
    "    )\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "X_train, y_train = generate_dataset(10_000)\n",
    "X_valid, y_valid = generate_dataset(2_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first training sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(12,), dtype=int32, numpy=array([0, 4, 0, 2, 4, 4, 4, 6, 6, 1, 4, 1], dtype=int32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And its correspond class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 14:53:58.410547: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-03 14:53:58.413387: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-03 14:53:58.415168: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-04-03 14:53:58.498041: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [10000,1]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-04-03 14:53:59.201569: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-03 14:53:59.206885: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-03 14:53:59.211026: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2024-04-03 14:54:01.052834: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-03 14:54:01.055512: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-03 14:54:01.057533: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - ETA: 0s - loss: 0.6929 - accuracy: 0.5130"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 14:54:09.708521: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [2000,1]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-04-03 14:54:10.329702: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2024-04-03 14:54:10.333947: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2024-04-03 14:54:10.337275: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 13s 28ms/step - loss: 0.6929 - accuracy: 0.5130 - val_loss: 0.6902 - val_accuracy: 0.4940\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.6819 - accuracy: 0.5563 - val_loss: 0.6715 - val_accuracy: 0.4845\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 9s 29ms/step - loss: 0.6566 - accuracy: 0.5957 - val_loss: 0.6561 - val_accuracy: 0.5560\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.6454 - accuracy: 0.5938 - val_loss: 0.6671 - val_accuracy: 0.6080\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.6374 - accuracy: 0.6051 - val_loss: 0.6476 - val_accuracy: 0.4650\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.6272 - accuracy: 0.6163 - val_loss: 0.6466 - val_accuracy: 0.6060\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.6079 - accuracy: 0.6407 - val_loss: 0.6006 - val_accuracy: 0.6505\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.5524 - accuracy: 0.7140 - val_loss: 0.5148 - val_accuracy: 0.7680\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.4730 - accuracy: 0.7752 - val_loss: 0.4181 - val_accuracy: 0.8095\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.3542 - accuracy: 0.8559 - val_loss: 0.2420 - val_accuracy: 0.9285\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 7s 21ms/step - loss: 0.2396 - accuracy: 0.9132 - val_loss: 0.2212 - val_accuracy: 0.9515\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.2295 - accuracy: 0.9165 - val_loss: 0.1212 - val_accuracy: 0.9635\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.1075 - accuracy: 0.9709 - val_loss: 0.0519 - val_accuracy: 0.9875\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.0399 - accuracy: 0.9893 - val_loss: 0.0101 - val_accuracy: 0.9980\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 8s 26ms/step - loss: 0.0066 - accuracy: 0.9997 - val_loss: 0.0038 - val_accuracy: 0.9995\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 8s 25ms/step - loss: 0.0701 - accuracy: 0.9807 - val_loss: 0.0134 - val_accuracy: 0.9975\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0064 - accuracy: 0.9994 - val_loss: 0.0057 - val_accuracy: 0.9995\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0322 - accuracy: 0.9901 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 7s 23ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 8s 24ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 8.8136e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "embed_size = 5\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.InputLayer(input_shape=[None], dtype=tf.int8, ragged=True),\n",
    "        tf.keras.layers.Embedding(input_dim=len(POSSIBLE_CHARS), output_dim=embed_size),\n",
    "        tf.keras.layers.GRU(30),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.02, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test our RNN on two tricky strings: the first one is bad while the second one is good. They only differ by the second ot last character. If the RNN get this right, it shows that it managed to understand that the second character is the same as the second to last character. This requires a fairly long short-term memory (which is why we used a GRU cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTBTSSSSSXXTTTTTTTVVETE\n",
      "BTBTSSSSSXXTTTTTTTVVEPE\n"
     ]
    }
   ],
   "source": [
    "good_string = generate_string(embed_reber_grammar)\n",
    "while len(good_string) < 20:\n",
    "    good_string = generate_string(embed_reber_grammar)\n",
    "bad_char = \"T\" if good_string[1] == \"P\" else \"P\"\n",
    "bad_string = list(good_string)\n",
    "bad_string[-2] = bad_char\n",
    "bad_string = \"\".join(bad_string)\n",
    "print(good_string, bad_string, sep=\"\\n\")\n",
    "test_strings = [bad_string, good_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "\n",
      "Estimated probability that these are Reber strings:\n",
      "BTBTSSSSSXXTTTTTTTVVEPE: 6.58%\n",
      "BTBTSSSSSXXTTTTTTTVVETE: 99.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 14:57:15.186813: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype variant and shape [2]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "X_test = tf.ragged.constant([string_to_ids(s) for s in test_strings], ragged_rank=1)\n",
    "\n",
    "y_proba = model.predict(X_test)\n",
    "print()\n",
    "print(\"Estimated probability that these are Reber strings:\")\n",
    "for index, string in enumerate(test_strings):\n",
    "    print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsonds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
