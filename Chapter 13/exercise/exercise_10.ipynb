{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 10:45:01.305242: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: *In this exercise you will download a dataset, split it, create a `tf.data.Dataset` to load it and preprocess it efficiently, then build and train a binary classification model containing an `Embedding` layer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: *Download the Large Movie Review Dataset, which contains 50,000 movie reviews from the Internet Movie Database (IMDb). The data is organized in two directories, train and test, each containing a pos subdirectory with 12,500 positive reviews and a neg subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words versions), but we will ignore them in this exercise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('datasets/aclImdb')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = \"https://ai.stanford.edu/~amaas/data/sentiment/\"\n",
    "filename = \"aclImdb_v1.tar.gz\"\n",
    "filepath = tf.keras.utils.get_file(\n",
    "    filename, root + filename, extract=True, cache_dir=\".\"\n",
    ")\n",
    "path = Path(filepath).with_name(\"aclImdb\")\n",
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a `tree()` function to view the structure of the `aclImdb` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree(path: Path, level=0, indent=4, max_files=3):\n",
    "    if level == 0:\n",
    "        print(f\"{path}/\")\n",
    "        level += 1\n",
    "    sub_paths = sorted(path.iterdir())\n",
    "    sub_dirs = [sub_path for sub_path in sub_paths if sub_path.is_dir()]\n",
    "    sub_files = [sub_path for sub_path in sub_paths if not sub_path in sub_dirs]\n",
    "    indent_str = \" \" * indent * level\n",
    "    for dir in sub_dirs:\n",
    "        print(f\"{indent_str}{dir.name}/\")\n",
    "        tree(dir, level + 1)\n",
    "    for file in sub_files[:max_files]:\n",
    "        print(f\"{indent_str}{file.name}\")\n",
    "    if len(sub_files) > max_files:\n",
    "        print(f\"{indent_str}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/aclImdb/\n",
      "    test/\n",
      "        neg/\n",
      "            0_2.txt\n",
      "            10000_4.txt\n",
      "            10001_1.txt\n",
      "            ...\n",
      "        pos/\n",
      "            0_10.txt\n",
      "            10000_7.txt\n",
      "            10001_9.txt\n",
      "            ...\n",
      "        labeledBow.feat\n",
      "        urls_neg.txt\n",
      "        urls_pos.txt\n",
      "    train/\n",
      "        neg/\n",
      "            0_3.txt\n",
      "            10000_4.txt\n",
      "            10001_4.txt\n",
      "            ...\n",
      "        pos/\n",
      "            0_9.txt\n",
      "            10000_8.txt\n",
      "            10001_10.txt\n",
      "            ...\n",
      "        unsup/\n",
      "            0_0.txt\n",
      "            10000_0.txt\n",
      "            10001_0.txt\n",
      "            ...\n",
      "        labeledBow.feat\n",
      "        unsupBow.feat\n",
      "        urls_neg.txt\n",
      "        ...\n",
      "    README\n",
      "    imdb.vocab\n",
      "    imdbEr.txt\n"
     ]
    }
   ],
   "source": [
    "tree(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 12500, 12500, 12500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def review_paths(dirpath: Path):\n",
    "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
    "\n",
    "\n",
    "train_pos = review_paths(path / \"train\" / \"pos\")\n",
    "train_neg = review_paths(path / \"train\" / \"neg\")\n",
    "test_valid_pos = review_paths(path / \"test\" / \"pos\")\n",
    "test_valid_neg = review_paths(path / \"test\" / \"neg\")\n",
    "\n",
    "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: *Split the test set into a validation set (15,000) and a test set (10,000)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "np.random.shuffle(test_valid_pos)\n",
    "\n",
    "test_pos = test_valid_pos[:5000]\n",
    "valid_pos = test_valid_pos[5000:]\n",
    "test_neg = test_valid_neg[:5000]\n",
    "valid_neg = test_valid_neg[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: *Use `tf.data` to create an efficient dataset for each set.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idmb_dataset(filepaths_positive, filepaths_negative):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for filepaths, label in [(filepaths_positive, 1), (filepaths_negative, 0)]:\n",
    "        for filepath in filepaths:\n",
    "            with open(filepath) as review_file:\n",
    "                reviews.append(review_file.read())\n",
    "            labels.append(label)\n",
    "    return tf.data.Dataset.from_tensor_slices(\n",
    "        (tf.constant(reviews), tf.constant(labels))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'First of all I\\'ve got to give it to the people that got this thing together. 9/11 is such a sensitive issue that making a movie that dares to be controversial about it takes a great deal of guts. It\\'s a shame, although not surprising, that the movie was banned in the US.<br /><br />That being said I think that the movie is superb with a couple of weak moments. The movie starts up with the Iranian segment which turns out to be somewhat reminiscent of Majid Majidi\\'s work (the absolutely beautiful \"heaven\\'s children\" and \"the color of paradise\"). Much like those 2 films the clip shows what happened through the innocent eyes of a class of Afgan refugees in Iran. Absolutely beautiful clip. Same goes for Sean Penn\\'s clip which is superb as well. But just as some of the clips are beutiful others are absolutely brutal. Alejandro Gonz\\xc3\\xa1les I\\xc3\\xb1\\xc3\\xa1rritu does the mexican clip and just like his gut-wrenching \"Amores perros\" he does it as brutal as he can. Most of the clip is a black screen with several sounds playing in the background. Those sounds are of the reporters and their shock as the second plane crashes, those who called home from the burning towers and left messages for their families, those who were angry....and he combines this with flashes of people jumping from the towers. A very hard clip to watch and one that you won\\'t forget.<br /><br />Some clips could turn out to be very hard to watch for Americans as some of the clips could be interpreted as \"you\\'re not the only ones that are suffering\". In particular the Egyptian and British clips that not only say that but turn the tables and say how much suffering the US has caused to other people.<br /><br />I will also make a special mention to the clips from Bosnia-Herzegovina, France, India and Japan (although this last one may seem terribly out of place it actually isn\\'t).<br /><br />However, not all the clips are great and I make a special mention on the clip from Israel which, in my opinion, is extremely weak. While the idea was good (a reporter is at the scene of a terrorist attack in Tel Aviv but his story gets bumped because of what happened in New York is something that a lot of us who live in countries at war can relate to) the realization is terrible. The clip ends up as just some entertainment reporter trying to get some air-time at all costs, a guy saying he\\'s a witness and hoping that he can go on TV, and soldiers and paramedics shouting just \"because\". The clip fails to capture any of the drama of such a situation.<br /><br />If you happen to have the chance to see it then you should, that is, unless you\\'re a conservative in which case you\\'d better stay out as you might get offended. But if you\\'re not then you might learn how many of us outside the US lived through 9/11.', shape=(), dtype=string)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "\n",
      "tf.Tensor(b\"Will Smith is one of the best actors of all time. I don't know how he does it. I read books constantly but if there is a movie with Will Smith I will watch it. He has a rare gift of pulling you into the movie and holding you there. This movie is one of the best movies I have seen yet. I watched it on Saturday and I still cannot get it out of my head. AMAZING and sad all at the same time. Thanks again Will. You must watch this movie from beginning to end to understand every part of the movie. You cannot miss a thing. Make sure you have plenty of Kleenex and your man/woman sitting next to you so you can cuddle. WELL Worth the money that you will pay to watch it.. don't wait for it to come on TV.\", shape=(), dtype=string)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "\n",
      "tf.Tensor(b'What I expected: A rather lame overly-stereotypical portrayal of a sports-mad guy and an equally lame stereotypical portrayal of the gal who likes him yet suffers while being second banana to his overly zealous support for his favorite sports team.<br /><br />What I got: An even-handed story where both guy and gal end up admitting -- to themselves and each other -- that they each have passions in their lives yet each can forgive the other to save the love they share.<br /><br />Sounds sappy but with the nonstop humor and terrific performances this story works! Barrymore is classic Barrymore: that perfect blend of sweet, strong, and adorable. We expect that from her and she delivered.<br /><br />But Fallon is the nice surprise in this film. He brings to the role the perfect blend of sports nut combined with the appreciation for the normal things in life, like caring about kids and his girlfriend. Fallon delivers his lines with subtle perfection. He can be caring (\"You just ran across the field for me!\") and in the same breath be obliviously blinded by his love for the Red Sox (\"How did the grass feel? Kinda spongy?\") at the same time. Fallon\\'s portrayal \"made\" the movie. Hopefully, this movie marks the beginning of a better film career for Fallon, something beyond the over-the-top sophomoric humor typical of SNL alums (i.e. Will Ferrell).<br /><br />In short, a movie that could have fallen victim to stereotypical male vs. female characters rose above that limitation and provided nonstop spot-on humorous lines, most delivered with brilliant subtlety by Fallon.<br /><br />Hey, I saw this with my wife -- not a baseball fan -- and she loved it as much as I did. It\\'s neither a \"Guy Flick\" nor a \"Chick Flick\". It\\'s a terrific make-you-laugh flick. Go see it!', shape=(), dtype=string)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 10:45:15.873204: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2024-03-21 10:45:15.933392: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [25000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    }
   ],
   "source": [
    "for X, y in idmb_dataset(train_pos, train_neg).take(3):\n",
    "    print(X)\n",
    "    print(y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 10:45:16.264517: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [25000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 10:45:28.300317: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [25000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for X, y in idmb_dataset(train_pos, train_neg).repeat(10):pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes 10 seconds to load the dataset and go through it 10 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But let's pretend the dataset does not fit in the memory.\n",
    "- Luckily, each review fits on just one line (they use `<br />` to indicate line breaks), so we can use `TextLineDataset` to read the reviews.\n",
    "- If they didn't, we would have to preprocess the input files (e.g., converting them to TFRecords).\n",
    "- For very large datasets, it would make sense to use a tool like Apache Beam for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idmb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):\n",
    "    dataset_neg = tf.data.TextLineDataset(\n",
    "        filepaths_negative, num_parallel_reads=n_read_threads\n",
    "    )\n",
    "    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n",
    "    dataset_pos = tf.data.TextLineDataset(\n",
    "        filepaths_positive, num_parallel_reads=n_read_threads\n",
    "    )\n",
    "    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n",
    "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 10:45:39.811716: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-03-21 10:45:39.811882: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 10:46:06.668716: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_8' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_8}}]]\n",
      "2024-03-21 10:46:06.668860: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for X, y in idmb_dataset(train_pos, train_neg).repeat(10): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now it takes about 19 seconds to go through the datasets 10 times. That's much slower, essentially because the dataset is not cached in RAM, so it must be reloaded at each epoch.\n",
    "- If you add `.cache()` just before `.repeat(10)`, you'll see that this implementation is about as fast as the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 10:46:31.056601: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_8' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_8}}]]\n",
      "2024-03-21 10:46:31.056774: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_8' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_8}}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 10:46:42.112297: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-03-21 10:46:42.112477: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_8' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_8}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 for X, y in idmb_dataset(train_pos, train_neg).cache().repeat(10):pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set = idmb_dataset(train_pos, train_neg).shuffle(25000, seed=24)\n",
    "train_set = train_set.batch(batch_size).prefetch(1)\n",
    "valid_set = idmb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\n",
    "test_set = idmb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: *Create a binary classification model, using a `TextVectorization` layer to preprocess each review.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we create a `TextVectorization` layer an adapt it to the full IDMB training set.\n",
    "- If the training set did not fit in RAM, we could just use a smaller sample of the training set by calling `train_set.take(500)`.\n",
    "- And we use TF-IDF for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 10:46:53.207869: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-03-21 10:46:53.208125: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_8' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_8}}]]\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 1000\n",
    "sample_reviews = train_set.map(lambda review, label: review)\n",
    "text_vectorization = tf.keras.layers.TextVectorization(max_tokens, output_mode=\"tf_idf\")\n",
    "text_vectorization.adapt(sample_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look at the first 10 words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the most common words in the reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 10:46:55.820633: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_8' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_8}}]]\n",
      "2024-03-21 10:46:55.820958: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [12500]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    777/Unknown - 3s 3ms/step - loss: 0.4237 - accuracy: 0.8227"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 10:46:58.856654: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [7500]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-03-21 10:46:58.856847: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [7500]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 4s 4ms/step - loss: 0.4232 - accuracy: 0.8230 - val_loss: 0.3664 - val_accuracy: 0.8483\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.8554"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 4s 4ms/step - loss: 0.3602 - accuracy: 0.8554 - val_loss: 0.3741 - val_accuracy: 0.8506\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.3179 - accuracy: 0.8700 - val_loss: 0.3917 - val_accuracy: 0.8398\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.2647 - accuracy: 0.8919 - val_loss: 0.3832 - val_accuracy: 0.8443\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 0.2081 - accuracy: 0.9168 - val_loss: 0.4723 - val_accuracy: 0.8335\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x703026d1f650>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        text_vectorization,\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get about 84.8% accuracy on the validation set after just the first epoch, but after that, the model makes no significant progress. We will do better in chapter 16. For now, we just want to practice preprocessing using `tf.data` and Keras preprocessing layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: *Add an `Embedding` layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To compute the mean embedding for each review, and multiply it by the square root of the number of words in the review, so we will need a little function.\n",
    "- For each sentence, this function need to compute $M \\times \\sqrt{N}$, where M is the mean of all the word embeddings in the sentence, where $M$ is the mean of all the word embeddings in the sentence (excluding padding tokens), and $N$ is the number of words in the sentence (also excluding padding tokens).\n",
    "- We can rewrite $M$ as $\\dfrac{S}{N}$, where $S$ is the sum of all word embeddings (it does not matter whether or not we include the padding tokens in this sum, as their representation is a zero vector anyways).\n",
    "- So the function must return $M \\times \\sqrt{N} = \\dfrac{S}{N} \\times \\sqrt{N} = \\dfrac{S}{\\sqrt{N}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_embedding(inputs):\n",
    "    # We count the number of words, or count the number of nonzero vectors\n",
    "    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n",
    "    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)\n",
    "    sqrt_n_words = tf.sqrt(tf.cast(n_words, tf.float32))\n",
    "    return tf.reduce_sum(inputs, axis=1) / sqrt_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[3.535534 , 4.9497476, 2.1213205],\n",
       "       [6.       , 0.       , 0.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_example = tf.constant(\n",
    "    [\n",
    "        [[1.0, 2.0, 3.0], [4.0, 5.0, 0.0], [0.0, 0.0, 0.0]],\n",
    "        [[6.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n",
    "    ]\n",
    ")\n",
    "compute_mean_embedding(another_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that this is correct. The first review contains 2 words, as the last token is a zero vector, which represents the `<pad>` token. Let's compute the mean embedding for these 2 words, and multiply the result by the square root of 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([3.535534 , 4.9497476, 2.1213202], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(another_example[0][:2], axis=0) * tf.sqrt(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. What about the second review, which contains only one word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([6., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(another_example[1][:1], axis=0) * tf.sqrt(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Great! Now we can train our final model. \n",
    "- It's the same as before, we just replace TF-IDF with ordinal encoding (`output_mode=\"int\"`) followed by an `Embedding` layer, followed by a `Lambda` layer that calls the `compute_mean_embedding()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 20\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "text_vectorization = tf.keras.layers.TextVectorization(max_tokens, output_mode=\"int\")\n",
    "text_vectorization.adapt(sample_reviews)\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        text_vectorization,\n",
    "        tf.keras.layers.Embedding(\n",
    "            input_dim=max_tokens,\n",
    "            output_dim=embedding_size,\n",
    "            mask_zero=True,  # <pad> token is  equivalent to zero vectors\n",
    "        ),\n",
    "        tf.keras.layers.Lambda(compute_mean_embedding),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: *Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 7s 7ms/step - loss: 0.4798 - accuracy: 0.7658 - val_loss: 0.3490 - val_accuracy: 0.8502\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3338 - accuracy: 0.8566 - val_loss: 0.3274 - val_accuracy: 0.8591\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3170 - accuracy: 0.8635 - val_loss: 0.3481 - val_accuracy: 0.8479\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.3122 - accuracy: 0.8670 - val_loss: 0.3291 - val_accuracy: 0.8567\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.3084 - accuracy: 0.8672 - val_loss: 0.3265 - val_accuracy: 0.8567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x702fd76ae690>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is just marginally (i.e., slightly) better using embeddings (but we will do better in chapter 16). The pipeline is fast enough (we optimized it earlier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: *Use TFDS to load the same dataset more easily: `tfds.load(\"imdb_reviews\")`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/tan/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026a6534023248b0a3739b7ed4160b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450f73786ac444fe8c0fe7e2d564f50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df32011215024a6eadce96e28f6c8dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a3cef7ef844de5beda52587ee431d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391f1bef85e34a07b6ad9e0e2f28db18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/tan/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete41HOQB/imdb_reviews-train.tfre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1cc6ca3a1c4cf590beea08b9e362c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c9831ca6f54f3fb2acfce491cec08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/tan/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete41HOQB/imdb_reviews-test.tfrec…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dad4359419349d7a1ded7137a9f38c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a157d9a90e3a4d4b9169c6ff04f5e0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/tan/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete41HOQB/imdb_reviews-unsupervis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /home/tan/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets = tfds.load(\"imdb_reviews\")\n",
    "train_set, test_set = datasets[\"train\"], datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 11:00:08.156896: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-03-21 11:00:08.157392: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2024-03-21 11:00:08.188401: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for example in train_set.take(1):\n",
    "    print(example[\"text\"])\n",
    "    print(example[\"label\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsonds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
