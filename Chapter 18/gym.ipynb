{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_environments(env, fig_size=(5, 4)):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    img = env.render()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    return img\n",
    "\n",
    "#plot_environments(env)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02727336,  0.18847767,  0.03625453, -0.26141977], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = 1\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_reward = 0\n",
    "    obs, info = env.reset(seed=episode)\n",
    "    for step in range(200):\n",
    "        action=basic_policy(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "    totals.append(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.698 8.389445512070509 24.0 63.0\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(totals),np.std(totals), np.min(totals), np.max(totals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(5, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env: gym.Env, obs, model: keras.Sequential, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, terminated, truncated, info = env.step(int(action))\n",
    "    if terminated:\n",
    "        env.reset(seed=42)\n",
    "    return obs, reward, terminated, truncated, grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env:gym.Env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs, info = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, terminated, truncated, grads = play_one_step(\n",
    "                env, obs, model, loss_fn\n",
    "            )\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor) \n",
    "                              for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return[(discount_reward - reward_mean)/ reward_std \n",
    "           for discount_reward in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes_per_iterations = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor iteration in range(n_iterations):\\n    all_rewards, all_grads = play_multiple_episodes(\\n        env, n_episodes_per_iterations, n_max_steps, model, loss_fn\\n    )\\n\\n    total_rewards = sum(map(sum, all_rewards))\\n    print(f\"\\rIteration: {iteration + 1}/{n_iterations},\"\\n          f\" mean rewards: {total_rewards / n_episodes_per_iterations:.1f}\", end=\"\")\\n\\n\\n    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\\n    \\n    all_means_grads = []\\n    for var_index in range(len(model.trainable_variables)):\\n        mean_grads = tf.reduce_mean(\\n            [final_reward * all_grads[episode_index][step][var_index]\\n             for episode_index, final_rewards in enumerate(all_final_rewards)\\n                for step, final_reward in enumerate(final_rewards)], \\n            axis=0\\n        )\\n        all_means_grads.append(mean_grads)\\n\\n    optimizer.apply_gradients(zip(all_means_grads, model.trainable_variables))\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_iterations, n_max_steps, model, loss_fn\n",
    "    )\n",
    "\n",
    "    total_rewards = sum(map(sum, all_rewards))\n",
    "    print(f\"\\rIteration: {iteration + 1}/{n_iterations},\"\n",
    "          f\" mean rewards: {total_rewards / n_episodes_per_iterations:.1f}\", end=\"\")\n",
    "\n",
    "\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "    \n",
    "    all_means_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                for step, final_reward in enumerate(final_rewards)], \n",
    "            axis=0\n",
    "        )\n",
    "        all_means_grads.append(mean_grads)\n",
    "\n",
    "    optimizer.apply_gradients(zip(all_means_grads, model.trainable_variables))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "#joblib.dump(model, \"my_model.pkl\")\n",
    "model = joblib.load(\"my_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run #1: 0 0 3 \n",
      "Run #2: 0 1 2 1 2 1 2 1 2 1 3 \n",
      "Run #3: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "Run #4: 0 3 \n",
      "Run #5: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "Run #6: 0 1 3 \n",
      "Run #7: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "Run #8: 0 0 0 1 2 1 2 1 3 \n",
      "Run #9: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
      "Run #10: 0 0 0 1 2 1 3 \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "transition_probabilities = [ # shape=[s, s']\n",
    "        [0.7, 0.2, 0.0, 0.1],  # from s0 to s0, s1, s2, s3\n",
    "        [0.0, 0.0, 0.9, 0.1],  # from s1 to s0, s1, s2, s3\n",
    "        [0.0, 1.0, 0.0, 0.0],  # from s2 to s0, s1, s2, s3\n",
    "        [0.0, 0.0, 0.0, 1.0]]  # from s3 to s0, s1, s2, s3\n",
    "\n",
    "n_max_steps = 1000  # to avoid blocking in case of an infinite loop\n",
    "terminal_states = [3]\n",
    "\n",
    "def run_chain(start_state):\n",
    "    current_state = start_state\n",
    "    for step in range(n_max_steps):\n",
    "        print(current_state, end=\" \")\n",
    "        if current_state in terminal_states:\n",
    "            break\n",
    "        current_state = np.random.choice(\n",
    "            range(len(transition_probabilities)),\n",
    "            p=transition_probabilities[current_state]\n",
    "        )\n",
    "    else: \n",
    "        print(\"...\", end=\"\")\n",
    "\n",
    "    print()\n",
    "\n",
    "for idx in range(10):\n",
    "    print(f\"Run #{idx + 1}: \", end=\"\")\n",
    "    run_chain(start_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],\n",
    "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],\n",
    "    [None, [0.8, 0.1, 0.1], None]\n",
    "]\n",
    "rewards = [\n",
    "    [[10, 0, 0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],\n",
    "    [[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, -50.0]],\n",
    "    [[0.0, 0.0, 0.0], [40.0, 0.0, 0.0], [0.0, 0.0, 0.0]]\n",
    "]\n",
    "possible_action = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values = np.full((3, 3), -np.inf)\n",
    "for state, action in enumerate(possible_action):\n",
    "    Q_values[state, action] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_action[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                transition_probabilities[s][a][sp] \n",
    "                * (reward[s][a][sp] + gamma * np.max(Q_prev[sp]))\n",
    "            for sp in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Q_values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    probas = transition_probabilities[state][action]\n",
    "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
    "    reward = rewards[state][action][next_state]\n",
    "    return next_state, reward\n",
    "\n",
    "def exploration_policy(state):\n",
    "    return np.random.choice(possible_action[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha0 = 0.05  #  initial learning rate\n",
    "decay = 0.05  #  learning rate decay\n",
    "gamma = 0.9  #  discount factor\n",
    "state = 0  #  initial state\n",
    "\n",
    "for iteration in range(10000):\n",
    "    action = exploration_policy(state)\n",
    "    next_state , reward = step(state, action)\n",
    "    next_value = np.max(Q_values[next_state])\n",
    "    alpha = alpha / (1 + iteration * decay)\n",
    "    Q_values[state, action] *= alpha\n",
    "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
    "    state = next_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
