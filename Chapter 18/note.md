*Reinforcement learning* is arguably one of the most exciting field of machine learning (also the one attracts me to ML), and also one the oldest. It has been around since 1950s, producing many interesting applications over the years, especially in games (e.g., *TD-Gammon*, a Backgammon-playing program), and in machine control, but seldom making the headline news. However, a revolution happened in [2013](https://arxiv.org/abs/1312.5602), when researchers from DeepMind demonstrated a system that could play any Atari game from scratch, eventually [outperforming human](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) in most of them, using only raw pixels as input and without any prior knowledge about the rule of the games. This was just the beginning, followed by the victory of their system AlphaGo against Lee Sedol, a professional player of the Go game, in March 2013 and against Ke Jie, the world champion, in May 2017. No program had ever close to beating a pro player of the game, let alone the world champion. Today, the field of RL is boiled with new ideas, with a huge range of applications.

How did DeepMind(Bought by Google in 2014) do such feats? With hindsight it seems simple: they applied the power of deep learning to the field of reinforcement learning, and it worked beyond their wildest dreams. In this chapter, we'll discover what reinforcement learning is and what it's good at, then present two of the most important techniques in deep reinforcement learning: policy gradients and deep Q-networks, including a discussion of Markov decision progress.

# Learning to Optimize Rewards

In reinforcement learning, a software *agent* makes *observations* and takes *actions* within the environment,a nd in return it receives *rewards* from the environment. Its objective is to learn how to maximize its expected rewards over time. You can think of positive rewards as pleasure, and negative rewards as pain (the term "reward" is a bit misleading here). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain.

This's quite a board setting, which can be applied to a wide variety of tasks. Here are a few examples:
- The agent can be the program controlling a robot. The environment then can be the real world, the agent observes the world through a set of *sensors*, such as cameras and touch sensors, and its actions consists of sending signals to active motors. It may be programmed to get positive rewards whenever it approaches the target destination, and negative rewards if it falls or waste time.
- The agent can be the the program playing the game of *Ms. Pac-Man*. In this case, the environment is a stimulation of the game, the actions are nine possible joysticks positions (upper left, down, center, and so on), the observations is the game screenshot, and the rewards is the game points.
- Similarly, the agent can be the program playing a board game such as Go. It will only be rewarded if it wins.
- The agent does not have to control a physically (or virtually) moving thing. For example, it cna be a thermostat, getting positive rewards whenever it is close to the target temperature and saves energy, and negative rewards when human needs to tweak the temperature, so the agent must learn to anticipate the human desires.
- The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the number of money earned.

Note that there may not be any positive reward at all. For example, the agent may move around in a maze, get a negative reward every second at every time step, so it had to find the exit as quick as possible.

There are many other applications for reinforcement learning, such as self-driving cars, recommender system, placing ads on a web page, or control where an image classification system should focus its attention.
