*Reinforcement learning* is arguably one of the most exciting field of machine learning (also the one attracts me to ML), and also one the oldest. It has been around since 1950s, producing many interesting applications over the years, especially in games (e.g., *TD-Gammon*, a Backgammon-playing program), and in machine control, but seldom making the headline news. However, a revolution happened in [2013](https://arxiv.org/abs/1312.5602), when researchers from DeepMind demonstrated a system that could play any Atari game from scratch, eventually [outperforming human](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) in most of them, using only raw pixels as input and without any prior knowledge about the rule of the games. This was just the beginning, followed by the victory of their system AlphaGo against Lee Sedol, a professional player of the Go game, in March 2013 and against Ke Jie, the world champion, in May 2017. No program had ever close to beating a pro player of the game, let alone the world champion. Today, the field of RL is boiled with new ideas, with a huge range of applications.

How did DeepMind(Bought by Google in 2014) do such feats? With hindsight it seems simple: they applied the power of deep learning to the field of reinforcement learning, and it worked beyond their wildest dreams. In this chapter, we'll discover what reinforcement learning is and what it's good at, then present two of the most important techniques in deep reinforcement learning: policy gradients and deep Q-networks, including a discussion of Markov decision progress.

# Learning to Optimize Rewards

In reinforcement learning, a software *agent* makes *observations* and takes *actions* within the environment,a nd in return it receives *rewards* from the environment. Its objective is to learn how to maximize its expected rewards over time. You can think of positive rewards as pleasure, and negative rewards as pain (the term "reward" is a bit misleading here). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain.

This's quite a board setting, which can be applied to a wide variety of tasks. Here are a few examples:
- The agent can be the program controlling a robot. The environment then can be the real world, the agent observes the world through a set of *sensors*, such as cameras and touch sensors, and its actions consists of sending signals to active motors. It may be programmed to get positive rewards whenever it approaches the target destination, and negative rewards if it falls or waste time.
- The agent can be the the program playing the game of *Ms. Pac-Man*. In this case, the environment is a stimulation of the game, the actions are nine possible joysticks positions (upper left, down, center, and so on), the observations is the game screenshot, and the rewards is the game points.
- Similarly, the agent can be the program playing a board game such as Go. It will only be rewarded if it wins.
- The agent does not have to control a physically (or virtually) moving thing. For example, it cna be a thermostat, getting positive rewards whenever it is close to the target temperature and saves energy, and negative rewards when human needs to tweak the temperature, so the agent must learn to anticipate the human desires.
- The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the number of money earned.

Note that there may not be any positive reward at all. For example, the agent may move around in a maze, get a negative reward every second at every time step, so it had to find the exit as quick as possible.

There are many other applications for reinforcement learning, such as self-driving cars, recommender system, placing ads on a web page, or control where an image classification system should focus its attention.

# Policy Search

The algorithm the agent used ot define its actions is called a *policy*. It's can be as simple as a bunch of hardcode if/else, to as complex as a whole deep stack of neural network. It must take observations as inputs and output the action to take.

![Reinforcement learning using a neural network policy](image.png)

The policy can be any algorithm you think about, and it doesn't have to be deterministic (we will talk about a probabilistic one soon). In fact, sometimes it does not even have to observe the environment at all! For instance, consider a robotic vacuum cleaner whose reward is the amount of dust it has colleted during 30 minutes. Its policy could be moving forward with some probability p, or randomly rotate left or right with probability 1-p. the rotation angle would be random from -r to r. Since this policy involves some randomness, it's called a *stochastic policy*. The robot will have an erratic trajectory, which guarantees it will eventually get to any place it can get and pick up all the dust. The question is how much dust it can picked up in 30 minutes?

How should you train this model? There are two *policy parameters* to tweak: the probability to move forward p and the angle range r. One possible is to try every possible combination, but this is not a good use of resources. An improved way is to try a bunch of random probability parameters and choose the combination that performs best. this is an example of *policy search*, in this case using a brute-force attack. When the *policy space* is too large (which is generally the case), finding a good set of parameters this way is like finding a needle in a big haystack.

![Four points in the policy space (left) and the agentâ€™s corresponding behavior (right)](image-1.png)

Another way is to use *genetic algorithm*. For example, you could randomly create a first generation of 100 policies and try them out, then "kill" the worst 80 policies (though it's maybe better to keep some of them, to persevere gene diversity) and make the 20 survivors produce 4 offsprings each. An offspring is a copy of its parent, plus some random variation. The surviving policies and their offsprings together run through the second run. if there is a single parent, this is called *asexual reproduction*. With two (or more) parents, it is called *sexual reproduction*. An offspring's genome (in this case is a set of policy parameter) is randomly composed of parts of its parents' genomes. You can continue to iterate though generations this way until you find a good enough policy. One interesting example of genetic algorithm sued for reinforcement learning is the [NeuroEvolution of Augmenting Topologies](https://neat-python.readthedocs.io/en/latest/neat_overview.html) (NEAT) algorithm. You can also check out this [video](), which implements and teaches a lot about this approach (this is my personal recommendation).

Yet another approach is to use optimization techniques, by evaluating the gradients of the rewards with regard to the policy parameters, then tweaking these parameter by following these gradients toward higher rewards. This is called *gradient ascent*: It's just like gradient descent, but in the opposite direction: maximizing instead of minimizing. We will talk about this approach in more detail later in this chapter. Going back to the vacuum cleaner example, we could slightly increase p and evaluate whether doing so increases the amount of dust picked up by the robot in 30 minutes; if it does, then increase p some more; if it doesn't, decrease it instead. We'll implement a popular PG (*policy gradient*) using TensorFlow, but first we need to create an environment for the agent ot live in - so it's time to introduce OpenAI Gymnasium.

# Introduction to OpenAI Gymnasium

One of the challenge of reinforcement learning is that in order to train an agent, you need a working environment. If you want a program that can learn to play an Atari game, you will need an Atari stimulator. If you want to program a walking robot, then the environment is the real world, and you can directly train your robot in your environment. However, this has some big limitations: If your robot fall of  a cliff, you can click Undo. You can't speed it up neither, as adding more computation pawer won't make the robot move any faster, and training 1,000 robots in parallel is way too expensive. In short, training is hard and expensive in real world, so you need a  *stimulated environment* at least for bootstrap training. For example, you might use [PyBullet]() or [MuJoCo]() for 3D physics stimulation.

OpenAI Gym is a toolkit that provides a wide variety of simulated environments (Atari games, board games, 2D and 3D physical stimulations, and so on), that you can use to train agents, compare them, or develop new RL algorithm. However, Gym has been moved to Gymnasium by the Farama Foundation, which luckily has backward compatibility with Gym, so we can download it instead and use `import gymnasium as gym` instead of `import gym`.

You can download Gymnasium using these commands:
```bash
%pip install -p -U gymnasium
%pip install -p -U gymnasium[classic_control, box2d, atari, accept-rom-license]
```
The `%` is only required in the notebook cell, you should remove it if running in terminal. The `-q` for *quiet*: it makes the output less verbose. The `-U` option stands for *upgrade*. The second command installs the library required to run various kind of environments. this includes classic environments from *control theory* - the science of controlling dynamical systems - such as balancing a pole on a cart. it also includes environment based on the Box2D library - a 2D physic engine for games. [You need to install `swig` beforehand](https://github.com/Farama-Foundation/Gymnasium/issues/662) however if you want to install Box2D, which can be done using `sudo apt-get install swig` in Linux. Lastly, it includes environments based on the Arcade Learning Environment (ALE), which is an emulator for Atari 2600 games. Several Atari game ROMs are downloaded automatically, and by running this code, you agree with Atari's ROM licenses. You can just use `gymnasium[all]` if you want all the dependencies. 

In the learning, I created a CartPole environment. This is a 2D stimulation in which a cart can be accelerate left or right in order to balance a pole placed on top of it. This is a classic control task.

The `gym.envs.registry` dictionary contains the names and specifications of all the variable environments.
After the environment is created, you must initialize it using the `reset()` method, optionally passing a random seed. This return the first observation. Observations depend on the type of environment. For the CartPole environment, each observation is a 1D NumPy array containing four floats representing the cart's horizontal position (`0.0` = center), its velocity (positive means right), the angle of the pole (`0.0` = vertical), and its angular velocity (positive means clockwise). The `reset()` method also returns a dictionary that may contain extra environment-specific information. This can be useful for debugging or training. For example, in many Atari environments, it contains the number of lives left. However, in the CartPole environment, this dictionary is empty.

Let's call the `render()` method to render this environment as an image. Since we set `sender_mode="rbg_array"` when creating this environment, the image will be returned as a NumPy array. You can then use Matplotlib's `imshow()` function to display this image, as usual.

You can use the `action_space` attribute to see what actions are possible. `Discrete(2)` means that the possible actions are integers 0 and 1, which representing accelerating left or right. Other environments may have additional discrete actions, or other kinds of actions (e.g., continuous).

The `step()` method executes the desired action and return five values:
- `obs`: This is the new observation. The cart is now moving toward the right (`obs[1] > 1`). The pole is still tilted toward the right (`obs[2] > 0`), but its angular velocity is now negative (`obs[3] < 0`), so will be likely to be titled toward the left in the next step.
- `reward`: In this environment, you get a reward of 1.0 at every step, no matter what you do, so the goal is to keep the episode running as long as possible.
- `done`: This value will be `True` when the episode is over. This will happen when the pole tilts too much, or goes off the screen, or after 200 steps (in this last case, you have won). After that, the environment must be reset before it can be used again.
- `truncated`: This value will be `True` when the episode is interrupted early, for example by an environment wrapper that imposes the maximum number of steps per episode (see [Gymnasium's documentation](https://gymnasium.farama.org/api/wrappers/) for more details on environment wrappers). Some RL algorithms treat truncated episodes differently from episodes finished normally (i.e., when `done` is `True`), but in this chapter we will treat them identically.
- `info`: This environment-specific dictionary may provide extra information, just like the one returned by the `reset()` method.
Once you have finished using an environment, you should call its `close()` method to free resources.

## A simple hardcoded policy

Let's hardcode a simple policy that accelerates left when the pole leaning toward the left and accelerates right when the pole leaning toward the right. We'll run this policy for 500 episodes and see its statistics. If you look at the result, even after 500 attempts, this policy never managed to keep the pole upright for more than 63 consecutive steps. Not good. If you look at the stimulation in this chapter's notebook, you'll see that the cart oscillates left and right more and more, until the pole tilts too much.

# Neural Network Policies

We can create a neural network policy. This neural network must take an observation as input, and output the action to be executed. In this case, this means the neural must have 4 neurons in the inputs layer, and only one in the output layer. More precisely, it will output an estimated probability for each action, and we will choose an action randomly based on these estimated probabilities. In the case of the CartPole environment, these are jst two possible actions, so we only need one output neuron. It will output the probability p of action 0 (left), and probability 1 - p of action 1 (right). For example, if the neuron output 0.7, then we choose action 0 with 70% probability, or action 1 with 30% probability.

![Neural network policy](image-2.png)

You may wonder why we are picking a random action based on the probabilities given by the neural network, rather than just picking the action with the highest score? This allows the model to find a good balance between *exploring* new actions and *exploiting* actions that known to be good. Here's an analogy: It's the first time you come to a new restaurant, and all the dished look equally good so you just pick a dish randomly. If it turns out to be great, you increase the chance to order it the next time, but never up to 100%, or else you may never try out other dishes, some of which may be even better than the one you tried. This *exploration/exploitation dilemma* is at the heart of reinforcement learning.

Also note that in this particular environment, the past actions and observations can be safely ignored, as each observation contains the full state of the environment. If there were some hidden states, then you might want to consider past actions and observations as well. For example, if we don't know the cart's velocity, but only the cart's position, then we need to know the previous position on order to estimate the current velocity. Another example is when the observations is noisy, hence we will need to know some past observations to estimate the most likely current state. TheCartPole is as simple as possible: The observations are noise-free, and they contain the environment's full states.

We can build a neural network using the `Sequential` API. You can see the implementation in the learning notebook. Because we just initialize it randomly, the neural network performed very badly.

# Evaluating Actions: The Credit Assignment Problem

We can create an architecture for the neural network, but how to train it? If we know the best action at each step, we could train the neural network as usual, by minimizing the cross entry between the estimate probability distribution and the target probability distribution. It would be just a regular supervised learning. However in reinforcement learning, the only guidance the agent gets is through reward, and rewards are typically sparse and delayed. For example, if the agent manages to balance the cart for 100 steps, how can it know which of the 100 actions were good, and which were bad? All it knows is the pole fell after the last action, but that last action is not fully responsible for the falling. This is called the *credit assignment problem*: when the agent gets a reward, it's hard for it to know which actions should get credited (or blamed) for it. Think of a dog getting a reward hours after it behaved well, will it understand that what it is rewarded for?

To tackle this problem, a common strategy is to evaluate an action based on the sum of all the rewards that comes after it, usually applying a *discount factor*, $\gamma$, at each step. This sum of discounted rewards is called the actions's *return*. If an agent decides to go right three times in a row and gets +10 reward after the first step, 0 after the second step, and finally -50 after the third step, then assuming we use the discount factor of $\gamma = 0.8$, the first action will have a return of $10 + \gamma \times 0 + \gamma^2 \times (-50) = -22$. If the discount factor is close to 0, then future rewards won't count too much compared to immediate rewards. Conversely, if the discount factor close to 1, then rewards far into the future will count almost as much as immediate rewards. Typical discount factors range from 0.9 to 0.99. With a discount factor of 0.95, rewards 13 into the future count roughly for half as much as immediate rewards (since $0.95^13 \approx 0.5$), while with a discount factor of 0.99, reward 69 steps into the future count for half as much as immediate rewards. In the CartPole environment, actions have fairly short-term effects, so choosing a discount of 0.95 seems reasonable.

![Computing an actionâ€™s return: the sum of discounted future rewards](image-3.png)

Of course, a good action may be followed by several of bad actions that cause the pole to fall quickly, which results in a good action getting a low return, makes the model to think that action is bad. Similarly, a good actor may sometimes star in a terrible movie. However, if we play the game enough times, on average good actions will get a higher return than bad ones. We want to estimate how much better or worse an action is, compared to the other possible actions, on average. This is called the *action advantage*. For this, we must run many episodes and normalize all the return values. After that, we can reasonably assume that actions with a negative advantage were bad while actions with positive advantage were good. Ok, now that we know how to evaluate each action, we're ready to train our first agent using policy gradients.

# Policy Gradient

As discussed earlier, PG algorithms optimize the parameters of a policy by following the gradients toward maximizing reward. One popular class of PG algorithms, called *REINFORCE algorithms*, was [introduced back in 1992](https://people.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf) by Ronald Williams. Here is a common variant:
- First, let the neural network policy play the game several times, and at every step, compute the gradients that would make the chosen action even more likely - but don't apply these gradients yet.
- Once you have run several episodes, compute each action's advantage using the method described in the last part.
- If an action's advantage is positive, it means that the action was probably good, and you want to apply the gradients computed earlier to make the action even more likely to be chosen in the future. However, if the action's advantage is negative, it means the action was probably bad, and you may want to apply the opposite gradients ot make this action less likely in the future. The solution is multiply each gradient vector by the corresponding action's advantage.
- Finally, compute the mean of all the resulting gradient vectors, and use it to perform a gradient descent step.

We will build from the ground up. First, we define a function to run the model for one step. We will pretend whatever actions it takes is the right one so that we can compute the loss and its gradients. These gradients will just be saved for a while, and we will modify them later depending on how good or bad the action turned out to be. We'll walk through the implementation in the learning notebook:
- Within the `GradientTape` block (see chapter 12), we start by giving the model observations. Since the model excepts a batch, we add one more axis in the outermost dimension to create a batch contains only one instance. This outputs the probability of going left.
- Next, we sample a random float between 0 and 1, and we check whether it is greater than `left_proba`. The action will be `False` with probability `left_proba`, or `True` with probability `1 - left_proba`. Once we cast this Boolean to an integer, the action will be 0 (left) or 1 (right).
- We now define the target probability of going left. As talked above, we pretend this action is the desired action, so it is 1 minus the action (cast to a float). If the action is 0 (left), then the target probability of going left will be 1. If the action is 1 (right), then the target probability of going left will be 0.
- Then we compute the loss using the given loss function, and we use the tape to compute the gradient fo the loss with regard to the model's trainable variables. You can think of these gradients as the way to increase the chance of doing this action with the model's policy. You don't know how good or bad this action will be, but follow these gradients will increase the chance of doing this action. These gradients will be tweaked later, depending on how good or bad the action turned out to be.
- Finally, we play the selected action, and we return the observations, reward, whether or not the episode is done or not, whether or not it is truncated or not, and the gradients we just computed.

We'll crate another function that will rely on the `play_one_step()` function to play multiple episodes, returning all the rewards and gradients for each episode and each step. The function will return a list of reward lists: one reward list per episode, containing one reward per step. It also returns a list of gradient lists: one gradient list per episode, each containing one tuple of gradients per step, and each tuple containing one gradient tensor per trainable variable.

The algorithm will use the `play_multiple_episodes()` function to play the game several times (e.g., 10 times), then it will go back and look at all the rewards, discount them and normalize them. To do that, we need a couple more functions; the first will compute the sum of future discounted rewards at each step, and the second will normalize all these discounted rewards (i.e., the returns) across all episodes by subtracting the mean and dividing by the standard deviation.

If you look at some test with the `discount_reward()` function, it returns exactly what we computed. You can also verify that the function `discount_and_normalize_reward()` function does normalize the action advantages in both episodes. Notice that the first action is much worse than the second, so its normalized advantage are all negative, while the second's are all positive. This means all the actions from the first episode are considered bad, while all from the second are considered good.

We are almost ready to run the algorithm. We just need to define some more hyperparameters: We will run 150 training iterations, run 10 episodes per iterations, and the maximum number of steps in each episode is 200. We'll use a discount factor of 0.95.

We also need an optimizer and a loss function. A Nadam optimizer with learning rate equals to 0.01 is fine, and since this is a binary classification task (there are two possible actions - left or right), we'll use binary cross-entropy loss function. Note that we pass the function itself, not an instance of it.

Now we can build the training loop:
- At each training iteration, we calculate all the rewards and gradients from every step of 10 episodes.
- We use `discount_and_normalize_rewards()` function to normalize all the actions advantage. This provides a hindsight of how actually these are actually good or bad.
- Next, we go through each trainable variable, for each of them, we computed the weighted mean of all the gradients times the normalized action associated to it, across every step in every episode.
- Finally, we use the optimizer to apply these mean gradient: the model's parameters will be tweaked in a way that will maximizing the action's advantage in all steps, thus hopefully the policy will be a bit better.

This code will train the neural network policy, and it will successfully learn to balance the pole on the cart. The mean reward per episode will be very close to 200, which means it's nearly won the game!

The simple policy gradients algorithm we just trained solved CartPole task, but it would not scale well to larger and more complex tasks. Indeed, it's highly *sample inefficient*, meaning it needs to explore the game for a very long time before it can make significant progress. This is due to the fact that it must run multiple episodes to estimate the advantage of each action, as we have seen. However, its's hte foundation of more powerful algorithms, such as *actor-critic* algorithms, which we will discuss briefly at end of this chapter.

Researchers try to find algorithms that work well even when the agent initially knows nothing about the environment. However, unless you're writing a paper, you should inject prior knowledge into the agent, as it would speed up training dramatically. For example, you know that the pole should be as vertical as possible, so you could add negative rewards proportional to the pole's angle. This will make rewards less parse and speed up training. Also, if you have already know a reasonably good policy (e.g., hardcoded), you may want to train the neural network to imitate it before using policy gradients to improve it.

# Markov Decision Processes

Whereas PG directly try to optimize the policy ot increase rewards, the algorithms we'll explore are less direct: the agent learns to estimate the expected return of each state or for each action in each state, then it uses this knowledge to decide how to act. To understand this class of algorithms, we must need to know *Markov decision processes* (MDPs). If you prefer an explanation using video, here is [my suggestion](https://www.youtube.com/watch?v=JGQe4kiPnrU).

In the early 20th century, the mathematician Andrey Markov studied stochastic processes with no memory, called *Markov chains*. Such a process had a fixed number of states,a nd it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to state s' is fixed, and it depends only on the pair (s, s'), not on past states.

![Example of a Markov chain](image-4.png)

Suppose the process start in state $s_0$, and there is a 70% chance it will remain in this state at the next step. Eventually it is bound to leave that state and never come back, since no state points back to state $s_0$. If it goes to state $s_1$, it most likely goes to state $s_2$ (with 90% chance), and immediately back to $s_1$ (with 100% chance). It may oscillate between these two states a number of times, but eventually it will go to state $s_3$ and remain there forever, since there's no way out: this is called a *terminal state*. Markov chains can have very different dynamics, and they are heavily used in thermodynamics, chemistry, statistics, and more.

Markov decision processes were first described in the 1950s by [Richard Bellman](https://www.jstor.org/stable/24900506). They resemble Markov chains, but with a twist: at each step, an agent can choose one of several possible actions, and the transition probabilities depend on the chosen action. Moreover, some states transitions return some rewards (positive or negative), and the agent's goal is to find a policy that will maximize the reward over time.

For example, the MSP represented below has three states (represented by circles) and up to three possible discrete actions at each step (represented by diamonds).

![Example of a Markov decision process](image-5.png)

If it starts in state $s_0$, the agent can choose between actions $a_0$, $a_1$, or $a_2$. If it chooses $a_1$, it just remains in $s_0$, and gains no reward. But if it chooses action $a_0$, it has 70% chance to gain a reward of +10 and remain in state $s_0$. It can choose to do it repeatedly, but eventually it will go to state $s_1$. In state $s_1$, it has two possible actions: $a_0$ or $a_2$. It can choose $a_0$ ot remain in state $s_1$, or choose $a_2$ to go to $s_2$ and get a negative reward of -50. In state $s_2$, it has no choice but chooses $a_1$, which has a probability of 80% to lead it back to $s_0$ and gain a reward of +40. Looking at this MDP, how do you know which action is the best at each state? In state $s_0$, it's easy to see that action $a_0$ is the best option, and we have no choice in state $s_2$ but to take action $a_1$, but in state $s_1$, it's not obvious whether the agent should stay put ($a_0$) or go through the fire ($a_2$).

Bellman found a way to estimate the *optimal state value* of any state $s$, noted $V^*(s)$, which is the sum of all discounted rewards the agent can expect on average after it reaches the state, assuming it acts optimally. He showed that if the agent acts optimally, then the *Bellman optimality equation* applies. This recursive equation says that if te agent acts optimally, then the optimal value of the current state is equal to the reward it gets on average after taking one optimal action, plus the expected optimal value of all possible next states that this action can lead to:
    $$V^*(s) = \underset{a}{max}\sum_{s'} T(s, a, s')[R(s, a, s') + \gamma.V^*(s')], \forall s $$
In this equation:
- We choose the optimal action at every state hence the variable to maximize is $a$.
- $T(s, a, s') $ is the transition probability when at state $s$, we choose action $a$, and we end up in state $s'$. For example, $T(s_2, a_1, s_0) =0.8$.
- $R(s, a, s') $ is the reward that the agent get when it goes from state $s$, by action $a$, and end up in state $s'$. For example. $R(s_2, a_1, s_0) = +40 $.
- $\gamma$ is the discount factor.

This equation leads directly to an algorithm that can precisely estimate the optimal state value of every possible state: first initialize all the state values estimations to zero, and then iteratively update them using the *value iteration* algorithm. A notable result is that, given enough time, these estimations are guaranteed to converge to the optimal state values, corresponding to the optimal policy:
    $$V_{k+1}(s) \leftarrow \underset{a}{max}\sum_{s'} T(s, a, s')[R(s, a, s') + \gamma.V_{k}(s')], \forall s $$
where $V_k(s)$ is the estimated value of state $s$ at the k-th iteration of the algorithm.

This algorithm is an example of *dynamic programming*, which breaks down a complex problem into tractable subproblems that can be tackled iteratively.

Knowing the optimal state value can be useful, in particular to evaluate a policy, but it does not give us the optimal policy for the agent. Luckily, Bellman found a very similar algorithm to estimate the optimal *state-action values*, generally called *Q-values* (quality values). The optimal Q-value of the state-action pair (s, a), noted $Q^*(s, a)$, is the sum of discounted future rewards the agent can expect on average after it reaches the state s and chooses action a, but before it sees the outcome of this action, assuming it acts optimally after that action.

You start by initializing all the Q-value estimates to zero, then you update them using the *Q-value iteration* algorithm:
    $$Q_{k+1}(s,a) \leftarrow \sum_{s'} T(s, a, s')[R(s, a, s') + \gamma.\underset{a'}{max}Q_{k}(s', a')], \forall (s, a) $$
Once you have the optimal Q-values, finding the optimal policy, noted $\pi^*(s)$, is trivial: when the agent is in state s, it should choose the action with the highest Q-value for that state: $\pi^*(s) = \underset{a}{argmax} Q^*(s, a)$.

First, we need to define the MDP. First, to know the probability of going from $s_2$ to $s_0$ after doing action $a_1$, we'll look up `transition_probability[2][1][0]`. Similarly, to get the corresponding reward, we'll look up `reward[2][1][0]`, which is +40 in our case. And to get list of possible actions in $s_2$, we'll look up `possible_actions[2]`, which is only $a_1$ in this case. Next, we must initialize all the Q-values to zero, expect for impossible actions, which will be set to $-\infty$.

Now we can run the Q-value iteration algorithm. It applies the update equation for Q-values repeatedly, to all !-values, for every state and every possible action. If you choose the discount factor of 0.9, then you can see the expected value in the learning notebook: for example, if we're in state $s_0$ and choose action $a_1$, the expected sum of discounted rewards is approximately 17.0. For each state, you can find the action with the highest Q-value, which gives us the optimal policy for this MDP.

Interestingly, if we increase the discount factor to 0.95, and we should walk through the fire! This means sense, since now we favor future rewards more, which means we can accept pain in the moment to gain in the future. 

# Temporal Difference Learning

Reinforcement learning problems with discrete actions can often be modeled as Markov decision processes, but the agent initially has no idea what the transition probabilities are (it does not $T(s, a, s')$), and it does not know what the rewards are going to be either (it does not know $R(s, a, s')$). It must experience each state and each transition at least once to know rewards, and it must experience them multiple times to have a reasonable estimates of the transition probabilities.

The *temporal difference (TD) learning* algorithm is very similar to the Q-value iteration algorithm, but tweaked to take into account the fact that the agent has only partial knowledge of the MDP. In general, we assume the model initially only knows the possible stases and actions, and nothing more. The agent uses an *exploration policy* - for example, a purely random policy - to explore the MDP, and as it progresses, the TD learning algorithm updates the estimates of the state value based on the transitions and rewards that are actually observed:
    $$V_{k+1}(s) \leftarrow (1-\alpha)V_k(s) + \alpha(r + \gamma.V_k(s'))$$
or equivalently:
    $$V_{k+1}(s) \leftarrow V_k(s) + \alpha.\delta(s, r, s')$$
    with $\delta(s, r, s') = r + \gamma.V_k(s') - V_k(s)$.
In this equation:
- $\alpha$ is the learning rate
- $r$ is the reward we get when leaving the state $s$, $\gamma$ is the discount factor
- $r + \gamma.V_K(s')$ is called the *TD target*
- $\delta(s, r, s')$ is called the *TD error*

A more concise way of writing the first form of this equation is to use the notation $a \underset{\alpha}{\leftarrow} b$, which means $a_{k+1} = (1-\alpha).a_k + \alpha.b_k$. So the first expression can be simplified as: $V(s) \underset{\alpha}{\leftarrow} r + \gamma.V(s')$.

TD learning has many similarities with stochastic gradient descent, including the fact that it handles one sample at a time. Moreover, just like SGD, it can only truly converge if you gradually reduce the learning rate; otherwise, it will keep bouncing around the optimum Q-values. 

This means we update the optimal value for each state using a moving average of the reward plus discount factor times the value of the best state value among possible next states. In the author's words, for each state $s$, this algorithm keeps tracks of a running average of the immediate rewards the agent get upon leaving that state, plus the rewards it expects to get later, assuming it acts optimally.

# Q-learning

Similarly, the Q-learning is an adaption of the Q-value iteration algorithm to the situation where the transition probabilities and the rewards are initially unknown. Q-learning works by watching the agent play (e.g., randomly) and gradually improving its estimates of the Q-values. Once it has accurate Q-value estimates (or close enough), then the optimal policy is just choosing the action that has the highest Q-value (i.e., the greedy policy):
    $$Q(s, a) \underset{\alpha}{\leftarrow} r + \gamma.\underset{a'}{\max} Q(s', a')$$

For each state-action pair (s, a), this algorithm keeps track of the running average of the rewards r the agent gets upon leaving the state $s$ with action $a$, plus the discounted future reward it expects to get. To estimate this sum, we take the maximum of the Q-value estimates for the next state s', since we assume the target policy will act optimally from then on.

Now we implement the Q-learning algorithm. First, we need to make the agent explore the environment. For this, we need a step function so that the agent can execute one action and get the resulting state and reward.

Next, we need to create the agent's exploration policy. This policy needs to visit every possible state many times, but since the search space is small, a simple random policy will be sufficient.

Next, we initialize the Q-values juts like before, and we're ready to run the Q-learning algorithm with learning rate decay (using power scheduling).

This algorithm will eventually converge to the optimal Q-values, but it may take many iterations, and possibly many of hyperparameter tuning. As you can see in the learning notebook, the Q-value iteration algorithm converges very quickly, in under 20 iterations, while the Q-learning algorithm takes about 8,000 iterations (up to 2 orders of magnitude!) to converge. Obviously, not knowing about the transition probability and the rewards makes finding the optimal policy significantly harder!

The Q-learning algorithm is called an *off-policy* algorithm because the policy being trained is not necessary the one used during training. For example, in our implementation, the policy being executed (the exploration policy) was completely random, while the policy being trained was never used. After training the optimal policy corresponds to systemically choosing the action with the highest Q-value. Conversely, the policy gradients algorithm is an *on-policy* algorithm: it explores the world using the policy being trained. It's somewhat surprising that the Q-learning can learning the optimal policy by just watching an agent acts randomly. Imagine learning to play golf while your teacher is a drunk monkey. How can we do better?

## Exploration Policies

Q-learning can only work if the exploration policy explores the MDP thoroughly enough, which means all possible stases and actions has all been explored enough times to have an estimates of the transition probability and rewards. Although a purely random policy is guaranteed to eventually visit all states and transitions many times, it may take an extreme long time to do so. Therefore, a better option is to use the *$\epsilon$-greedy policy*: at each step it acts randomly with probability $\epsilon$, or greedily with probability $1-\epsilon$ (i.e., choosing the action with the highest Q-value). The advantage of the $\epsilon$-greedy policy (compared to a completely random policy) is that it will spend more and more time exploring the interesting parts of the environment, as the Q-value estimates get better and better, while spend some time visiting unknown parts of the MDP. It is quite common to start with a high value for $\epsilon$, for example $\epsilon=1.0$, which means the model will just explore, and then gradually reduce it to a small value, for example $\epsilon=0.05$, which means the model will mostly just exploit.

Alternatively, instead of relying only on chance for exploration, another approach is to encourage the exploration policy to try actions that it has not tried much before. This can be added as a bonus to the Q-value estimates, as shown below:
    $$Q(s, a) \underset{\alpha}{\leftarrow} r + \gamma.\underset{a'}{\max} f(Q(s', a'), N(s', a'))$$
In this equation:
- $N(s', a')$ counts the number of times the action a' was chosen in state s'.
- $f(Q, N)$ is an *exploration function*, such as $f(Q, N) = Q + k / (1 + N)$, where $k$ is a curiosity hyperparameter that measures how much the agent is attracted to the unknown.

## Approximate Q-learning and Deep Q-Learning

The main problem with Q-learning is that it does not scale well to large (oe even medium) MDPs with many stases and actions. For example, suppose you wanted to use Q-learning to train an agent to play *Ms. Pac-Man*. There are about 240 pellets that Ms. Pac-Man cat eat, each of which can be present or absent (i.e., already eaten). SO the number of possible states are $2^{240} \approx 10^{72}$. And if you add all the possible combinations of positions of all the ghosts and Ms.Pac-man, the number of possible states becomes larger than the number of atoms in our planets, make it completely impractical to keep track of an estimate for every single Q-value.

The solution is ot find a function $Q_{\theta}(s, a)$ that approximates the Q-value of any state-action pair $(s, a)$ using a manageable  number of parameters (given by the parameter vector $\theta$). This is called *approximate Q-learning*. For year, it's recommended to use linear combination of handcrafted features (such as the distances to the ghosts, their directions, and so on) to estimate Q-values, but in 2013, [DeepMind]() showed that using deep neural network can work much better, especially for complex problems, and it does not require any feature engineering. A DNN used to estimate Q-value is called a *deep Q-network* (DQN), and using DQN for approximate Q-learning is called *deep Q-learning*.

How to train a DQN? We need to approximate the Q-value using the DQN for a given state-action pair $(s, a)$. Thanks to Bellman, we know we want this approximated Q-value to be as close as possible to the reward we actually observe after playing action a in state s, plus the discounted value of playing optimally from then on. To estimate this future Q-value, we can just execute the DQN on the nets state s', over all possible actions a', and get the approximate future Q-value for each action. We then pick the highest (since we assume we'll play optimally) and discount it, which gives us an estimate of the sum of future discounted rewards. By summing the reward $r$ and the future discounted value estimate, we get a target Q-value $y(s, a)$ for the state-action pair $(s, a)$:
    $$y(s, a) = r + \gamma . \underset{a'}{max} Q_\theta(s', a') $$
With this target Q-value, we can run a training, using any gradient descent algorithm you want. Specifically, we generally want to minimize the squared error between the estimated Q-value $Q_\theta(s, a)$ and the target Q-value $y(s, a)$, or the Huber loss to reduce the algorithm's sensitivity to larger errors.

## Implementing Deep Q-Learning

The first thing we need is the DQN. In theory, we need a neural network that takes a state-action pair as input,a and output an approximate Q-value. However, in practice, it's much more efficient ot use a neural network that takes only a state as input and outputs an approximate Q-value for each possible action. To solve the CartPole environment, we do not need a complex architecture: a few hidden layers will be fine.

To select an action using this DQN, we pick the action with the highest predicted Q-value. To ensure that the agent explores the environment, we'll use an $\epsilon$-greedy policy (i.e., we'll choose a random action with probability $\epsilon$).

Instead of training the DQN based only on the latest experiences, we will store all experiences in a *replay buffer* (or *replay memory*), and we will sample a random training batch form it at each training iteration. This helps reduce the correlations between the experiences in a training batch. You can think of it as the model thinks of past situations and try to do it again, not only think about the current situation, thus tremendously help training. For this, we use a doubled-ended queue (`deque`) instance from the built-in `collections` package.

A double-ended queue is a queue element that can be efficiently added to or removed from on both ends. Inserting and deleting items from the ends of the queue is very fast, but random access can be very slow when the queues gets long. If you need a very large replay buffer, you should consider a circular buffer instead, or check out DeepMind's [Reverb library](https://github.com/google-deepmind/reverb).

Each experience will be composed of six elements: an observation of the current state $s$, the action $a$ that the agent take, the resulting reward $r$, the next state $s'$ it reached, a Boolean indicating whether or not the episode need at this point (`done`), and another Boolean indicating whether the episode truncated at this point. We will need a small function to sample a random batch of experiences from the replay buffer. It will return six NumPy arrays, corresponding to six experience elements.

We also create a function to play a single step using the $\epsilon$-greedy policy, then store the resulting experience in the replay buffer.

Finally, we'll create one last function to sample a batch of experiences from the replay buffer and perform a single gradient descent step on this batch. Here is the breakdown:
- We start by sampling some experiences from the replay buffer and use the model to predict the Q-values of all the position action in the next state. Since we assume that the agent will be playing optimally, we only keep track of the maximum Q-value. Next, we use the equation above to compute the target Q-value for each experience's state-action pair.
- We want to the DQN to compute the Q-value for each experienced state-action pair, but the DQN will also output the Q-values for all the possible actions, not only the one was actually chosen by the agent. So we need to mask out all the Q-values we do not need. The `tf.one_hot()` function makes it possible by convert an array of actions into an one-hot matrix. For example, if we pass `[1, 1, 0]`, then we will receive `[[0, 1], [0, 1], [1, 0]]`, which is just the mask we want. We can then multiply the DQN's outputs with this mask, and this will zero out all the action we do not want. We sum it up over axis 1 to get rid of these zeros, keeping only the Q-values we need. This gives us the `Q_values` tensor containing one predicted Q-value for each experience in the batch.
- Next, we compute the loss: it's the mean squared error between the target and the predicted Q-values for the experienced state-action pairs.
- Finally, we perform a gradient descent step to minimize the loss with regard to all trainable variables of the model.

That's the hardest part! We have two pashes, which I call explore phase and thinking phase. First we explore a lot of states using our current DQN, then draw randomly some experiences and update the DQN using them. At each step, first we compute the `epsilon` value for the $\epsilon$-greedy policy: it will go from 1 down to 0.01, linearly, in 500 episodes. Then we use the `play_one_step()` function , which will use the $\epsilon$-greedy policy to pick an action , perform it and record the experience in the replay buffer. If the episode is done or truncated, we stop the episode. Finally, after we get past 50 episode, we call the `training_step()` function to train the model on a batch from the replay buffer. The reason we play many episodes without training is to fill up the replay buffer: if we don't explore enough, there won't be so much diversity in the replay buffer.

You can look at the total reward the agent gets after each episode in the learning notebook. As you can see, the algorithm took a while to start learning anything, in part because $\epsilon$ was very high at the beginning. The the training was just erratic: it first reached the max reward at around episode 320, but it immediately dropped, then it bounced up and down a few times, and soon after it looks like it had stabilized near the max reward, at around 360 episode, only to again dropped dramatically. This is called *catastrophic forgetting*, and is one of the big problems facing virtually any RL algorithms: as the agent explores the environment, it updates its policy, but what it learns in one part of the environment may break what it learned earlier in other parts of the environment. The experiences are quite correlated, and the learning environment keeps changing - this is not ideal for gradient descent. If you increase the size of the replay buffer, the algorithm will be less subject to this problem. Tuning the learning rate may also help. But the truth is, reinforcement learning is hard: training is often unstable, and you may need to try many hyperparameter values and random seeds before you find a combination that works well. For example, you can try changing the activation function from `"elu"` to `"relu"`, the performance will be much lower.

Reinforcement learning is notoriously difficult, largely because of the training instabilities and huge sensitive to [initial hyperparameters and random seeds](https://www.alexirpan.com/2018/02/14/rl-hard.html). As the researcher Andrej Karpathy put it, "[Supervised learning] wants to work. [...] RL must be forced to work". You will need time, patience, perseverance, and a bit of luck. This is a major reason why RL is not widely adopted as regular deep learning (e.g., convolutional nets). But there are still a few applications in real world, beyond AlphaGo and Atari games: for example, Google uses RL to optimize its datacenter costs, asn it is used in some robotic applications, for hyperparameter tuning, and in recommender systems.

You might wonder why don't we plot the loss. It turns that loss is a poor indicator of the model's performance. The loss might do down, yet the agent might perform worse (eg, this can happen when the agent is stuck in one small region of the environment, and the DQN starts overfitting this region). Conversely, the loss could go up, yet the agent might perform better (e.g., the DQN was underestimating the Q-values and it starts correctly increasing its predictions, the agent will likely perform better, get mote rewards, but the loss might increase because the DQN also sets the target, which will be larger, too). So, it's preferable to plot the rewards.