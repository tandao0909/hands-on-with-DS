*Reinforcement learning* is arguably one of the most exciting field of machine learning (also the one attracts me to ML), and also one the oldest. It has been around since 1950s, producing many interesting applications over the years, especially in games (e.g., *TD-Gammon*, a Backgammon-playing program), and in machine control, but seldom making the headline news. However, a revolution happened in [2013](https://arxiv.org/abs/1312.5602), when researchers from DeepMind demonstrated a system that could play any Atari game from scratch, eventually [outperforming human](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) in most of them, using only raw pixels as input and without any prior knowledge about the rule of the games. This was just the beginning, followed by the victory of their system AlphaGo against Lee Sedol, a professional player of the Go game, in March 2013 and against Ke Jie, the world champion, in May 2017. No program had ever close to beating a pro player of the game, let alone the world champion. Today, the field of RL is boiled with new ideas, with a huge range of applications.

How did DeepMind(Bought by Google in 2014) do such feats? With hindsight it seems simple: they applied the power of deep learning to the field of reinforcement learning, and it worked beyond their wildest dreams. In this chapter, we'll discover what reinforcement learning is and what it's good at, then present two of the most important techniques in deep reinforcement learning: policy gradients and deep Q-networks, including a discussion of Markov decision progress.

# Learning to Optimize Rewards

In reinforcement learning, a software *agent* makes *observations* and takes *actions* within the environment,a nd in return it receives *rewards* from the environment. Its objective is to learn how to maximize its expected rewards over time. You can think of positive rewards as pleasure, and negative rewards as pain (the term "reward" is a bit misleading here). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain.

This's quite a board setting, which can be applied to a wide variety of tasks. Here are a few examples:
- The agent can be the program controlling a robot. The environment then can be the real world, the agent observes the world through a set of *sensors*, such as cameras and touch sensors, and its actions consists of sending signals to active motors. It may be programmed to get positive rewards whenever it approaches the target destination, and negative rewards if it falls or waste time.
- The agent can be the the program playing the game of *Ms. Pac-Man*. In this case, the environment is a stimulation of the game, the actions are nine possible joysticks positions (upper left, down, center, and so on), the observations is the game screenshot, and the rewards is the game points.
- Similarly, the agent can be the program playing a board game such as Go. It will only be rewarded if it wins.
- The agent does not have to control a physically (or virtually) moving thing. For example, it cna be a thermostat, getting positive rewards whenever it is close to the target temperature and saves energy, and negative rewards when human needs to tweak the temperature, so the agent must learn to anticipate the human desires.
- The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the number of money earned.

Note that there may not be any positive reward at all. For example, the agent may move around in a maze, get a negative reward every second at every time step, so it had to find the exit as quick as possible.

There are many other applications for reinforcement learning, such as self-driving cars, recommender system, placing ads on a web page, or control where an image classification system should focus its attention.

# Policy Search

The algorithm the agent used ot define its actions is called a *policy*. It's can be as simple as a bunch of hardcode if/else, to as complex as a whole deep stack of neural network. It must take observations as inputs and output the action to take.

![Reinforcement learning using a neural network policy](image.png)

The policy can be any algorithm you think about, and it doesn't have to be deterministic (we will talk about a probabilistic one soon). In fact, sometimes it does not even have to observe the environment at all! For instance, consider a robotic vacuum cleaner whose reward is the amount of dust it has colleted during 30 minutes. Its policy could be moving forward with some probability p, or randomly rotate left or right with probability 1-p. the rotation angle would be random from -r to r. Since this policy involves some randomness, it's called a *stochastic policy*. The robot will have an erratic trajectory, which guarantees it will eventually get to any place it can get and pick up all the dust. The question is how much dust it can picked up in 30 minutes?

How should you train this model? There are two *policy parameters* to tweak: the probability to move forward p and the angle range r. One possible is to try every possible combination, but this is not a good use of resources. An improved way is to try a bunch of random probability parameters and choose the combination that performs best. this is an example of *policy search*, in this case using a brute-force attack. When the *policy space* is too large (which is generally the case), finding a good set of parameters this way is like finding a needle in a big haystack.

![Four points in the policy space (left) and the agentâ€™s corresponding behavior (right)](image-1.png)

Another way is to use *genetic algorithm*. For example, you could randomly create a first generation of 100 policies and try them out, then "kill" the worst 80 policies (though it's maybe better to keep some of them, to persevere gene diversity) and make the 20 survivors produce 4 offsprings each. An offspring is a copy of its parent, plus some random variation. The surviving policies and their offsprings together run through the second run. if there is a single parent, this is called *asexual reproduction*. With two (or more) parents, it is called *sexual reproduction*. An offspring's genome (in this case is a set of policy parameter) is randomly composed of parts of its parents' genomes. You can continue to iterate though generations this way until you find a good enough policy. One interesting example of genetic algorithm sued for reinforcement learning is the [NeuroEvolution of Augmenting Topologies](https://neat-python.readthedocs.io/en/latest/neat_overview.html) (NEAT) algorithm. You can also check out this [video](), which implements and teaches a lot about this approach (this is my personal recommendation).

Yet another approach is to use optimization techniques, by evaluating the gradients of the rewards with regard to the policy parameters, then tweaking these parameter by following these gradients toward higher rewards. This is called *gradient ascent*: It's just like gradient descent, but in the opposite direction: maximizing instead of minimizing. We will talk about this approach in more detail later in this chapter. Going back to the vacuum cleaner example, we could slightly increase p and evaluate whether doing so increases the amount of dust picked up by the robot in 30 minutes; if it does, then increase p some more; if it doesn't, decrease it instead. We'll implement a popular PG (*policy gradient*) using TensorFlow, but first we need to create an environment for the agent ot live in - so it's time to introduce OpenAI Gymnasium.

# Introduction to OpenAI Gymnasium

One of the challenge of reinforcement learning is that in order to train an agent, you need a working environment. If you want a program that can learn to play an Atari game, you will need an Atari stimulator. If you want to program a walking robot, then the environment is the real world, and you can directly train your robot in your environment. However, this has some big limitations: If your robot fall of  a cliff, you can click Undo. You can't speed it up neither, as adding more computation pawer won't make the robot move any faster, and training 1,000 robots in parallel is way too expensive. In short, training is hard and expensive in real world, so you need a  *stimulated environment* at least for bootstrap training. For example, you might use [PyBullet]() or [MuJoCo]() for 3D physics stimulation.

OpenAI Gym is a toolkit that provides a wide variety of simulated environments (Atari games, board games, 2D and 3D physical stimulations, and so on), that you can use to train agents, compare them, or develop new RL algorithm. However, Gym has been moved to Gymnasium by the Farama Foundation, which luckily has backward compatibility with Gym, so we can download it instead and use `import gymnasium as gym` instead of `import gym`.

You can download Gymnasium using these commands:
```bash
%pip install -p -U gymnasium
%pip install -p -U gymnasium[classic_control, box2d, atari, accept-rom-license]
```
The `%` is only required in the notebook cell, you should remove it if running in terminal. The `-q` for *quiet*: it makes the output less verbose. The `-U` option stands for *upgrade*. The second command installs the library required to run various kind of environments. this includes classic environments from *control theory* - the science of controlling dynamical systems - such as balancing a pole on a cart. it also includes environment based on the Box2D library - a 2D physic engine for games. [You need to install `swig` beforehand](https://github.com/Farama-Foundation/Gymnasium/issues/662) however if you want to install Box2D, which can be done using `sudo apt-get install swig` in Linux. Lastly, it includes environments based on the Arcade Learning Environment (ALE), which is an emulator for Atari 2600 games. Several Atari game ROMs are downloaded automatically, and by running this code, you agree with Atari's ROM licenses. You can just use `gymnasium[all]` if you want all the dependencies. 

In the learning, I created a CartPole environment. This is a 2D stimulation in which a cart can be accelerate left or right in order to balance a pole placed on top of it. This is a classic control task.

The `gym.envs.registry` dictionary contains the names and specifications of all the variable environments.
After the environment is created, you must initialize it using the `reset()` method, optionally passing a random seed. This return the first observation. Observations depend on the type of environment. For the CartPole environment, each observation is a 1D NumPy array containing four floats representing the cart's horizontal position (`0.0` = center), its velocity (positive means right), the angle of the pole (`0.0` = vertical), and its angular velocity (positive means clockwise). The `reset()` method also returns a dictionary that may contain extra environment-specific information. This can be useful for debugging or training. For example, in many Atari environments, it contains the number of lives left. However, in the CartPole environment, this dictionary is empty.

Let's call the `render()` method to render this environment as an image. Since we set `sender_mode="rbg_array"` when creating this environment, the image will be returned as a NumPy array. You can then use Matplotlib's `imshow()` function to display this image, as usual.

You can use the `action_space` attribute to see what actions are possible. `Discrete(2)` means that the possible actions are integers 0 and 1, which representing accelerating left or right. Other environments may have additional discrete actions, or other kinds of actions (e.g., continuous).

The `step()` method executes the desired action and return five values:
- `obs`: This is the new observation. The cart is now moving toward the right (`obs[1] > 1`). The pole is still tilted toward the right (`obs[2] > 0`), but its angular velocity is now negative (`obs[3] < 0`), so will be likely to be titled toward the left in the next step.
- `reward`: In this environment, you get a reward of 1.0 at every step, no matter what you do, so the goal is to keep the episode running as long as possible.
- `done`: This value will be `True` when the episode is over. This will happen when the pole tilts too much, or goes off the screen, or after 200 steps (in this last case, you have won). After that, the environment must be reset before it can be used again.
- `truncated`: This value will be `True` when the episode is interrupted early, for example by an environment wrapper that imposes the maximum number of steps per episode (see [Gymnasium's documentation](https://gymnasium.farama.org/api/wrappers/) for more details on environment wrappers). Some RL algorithms treat truncated episodes differently from episodes finished normally (i.e., when `done` is `True`), but in this chapter we will treat them identically.
- `info`: This environment-specific dictionary may provide extra information, just like the one returned by the `reset()` method.
Once you have finished using an environment, you should call its `close()` method to free resources.

## A simple hardcoded policy

Let's hardcode a simple policy that accelerates left when the pole leaning toward the left and accelerates right when the pole leaning toward the right. We'll run this policy for 500 episodes and see its statistics. If you look at the result, even after 500 attempts, this policy never managed to keep the pole upright for more than 63 consecutive steps. Not good. If you look at the stimulation in this chapter's notebook, you'll see that the cart oscillates left and right more and more, until the pole tilts too much.

# Neural Network Policies

We can create a neural network policy. This neural network must take an observation as input, and output the action to be executed. In this case, this means the neural must have 4 neurons in the inputs layer, and only one in the output layer. More precisely, it will output an estimated probability for each action, and we will choose an action randomly based on these estimated probabilities. In the case of the CartPole environment, these are jst two possible actions, so we only need one output neuron. It will output the probability p of action 0 (left), and probability 1 - p of action 1 (right). For example, if the neuron output 0.7, then we choose action 0 with 70% probability, or action 1 with 30% probability.

![Neural network policy](image-2.png)

You may wonder why we are picking a random action based on the probabilities given by the neural network, rather than just picking the action with the highest score? This allows the model to find a good balance between *exploring* new actions and *exploiting* actions that known to be good. Here's an analogy: It's the first time you come to a new restaurant, and all the dished look equally good so you just pick a dish randomly. If it turns out to be great, you increase the chance to order it the next time, but never up to 100%, or else you may never try out other dishes, some of which may be even better than the one you tried. This *exploration/exploitation dilemma* is at the heart of reinforcement learning.

Also note that in this particular environment, the past actions and observations can be safely ignored, as each observation contains the full state of the environment. If there were some hidden states, then you might want to consider past actions and observations as well. For example, if we don't know the cart's velocity, but only the cart's position, then we need to know the previous position on order to estimate the current velocity. Another example is when the observations is noisy, hence we will need to know some past observations to estimate the most likely current state. TheCartPole is as simple as possible: The observations are noise-free, and they contain the environment's full states.

We can build a neural network using the `Sequential` API. You can see the implementation in the learning notebook. Because we just initialize it randomly, the neural network performed very badly.