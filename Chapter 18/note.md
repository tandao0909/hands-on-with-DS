*Reinforcement learning* is arguably one of the most exciting field of machine learning (also the one attracts me to ML), and also one the oldest. It has been around since 1950s, producing many interesting applications over the years, especially in games (e.g., *TD-Gammon*, a Backgammon-playing program), and in machine control, but seldom making the headline news. However, a revolution happened in [2013](https://arxiv.org/abs/1312.5602), when researchers from DeepMind demonstrated a system that could play any Atari game from scratch, eventually [outperforming human](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) in most of them, using only raw pixels as input and without any prior knowledge about the rule of the games. This was just the beginning, followed by the victory of their system AlphaGo against Lee Sedol, a professional player of the Go game, in March 2013 and against Ke Jie, the world champion, in May 2017. No program had ever close to beating a pro player of the game, let alone the world champion. Today, the field of RL is boiled with new ideas, with a huge range of applications.

How did DeepMind(Bought by Google in 2014) do such feats? With hindsight it seems simple: they applied the power of deep learning to the field of reinforcement learning, and it worked beyond their wildest dreams. In this chapter, we'll discover what reinforcement learning is and what it's good at, then present two of the most important techniques in deep reinforcement learning: policy gradients and deep Q-networks, including a discussion of Markov decision progress.

# Learning to Optimize Rewards

In reinforcement learning, a software *agent* makes *observations* and takes *actions* within the environment,a nd in return it receives *rewards* from the environment. Its objective is to learn how to maximize its expected rewards over time. You can think of positive rewards as pleasure, and negative rewards as pain (the term "reward" is a bit misleading here). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain.

This's quite a board setting, which can be applied to a wide variety of tasks. Here are a few examples:
- The agent can be the program controlling a robot. The environment then can be the real world, the agent observes the world through a set of *sensors*, such as cameras and touch sensors, and its actions consists of sending signals to active motors. It may be programmed to get positive rewards whenever it approaches the target destination, and negative rewards if it falls or waste time.
- The agent can be the the program playing the game of *Ms. Pac-Man*. In this case, the environment is a stimulation of the game, the actions are nine possible joysticks positions (upper left, down, center, and so on), the observations is the game screenshot, and the rewards is the game points.
- Similarly, the agent can be the program playing a board game such as Go. It will only be rewarded if it wins.
- The agent does not have to control a physically (or virtually) moving thing. For example, it cna be a thermostat, getting positive rewards whenever it is close to the target temperature and saves energy, and negative rewards when human needs to tweak the temperature, so the agent must learn to anticipate the human desires.
- The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the number of money earned.

Note that there may not be any positive reward at all. For example, the agent may move around in a maze, get a negative reward every second at every time step, so it had to find the exit as quick as possible.

There are many other applications for reinforcement learning, such as self-driving cars, recommender system, placing ads on a web page, or control where an image classification system should focus its attention.

# Policy Search

The algorithm the agent used ot define its actions is called a *policy*. It's can be as simple as a bunch of hardcode if/else, to as complex as a whole deep stack of neural network. It must take observations as inputs and output the action to take.

![Reinforcement learning using a neural network policy](image.png)

The policy can be any algorithm you think about, and it doesn't have to be deterministic (we will talk about a probabilistic one soon). In fact, sometimes it does not even have to observe the environment at all! For instance, consider a robotic vacuum cleaner whose reward is the amount of dust it has colleted during 30 minutes. Its policy could be moving forward with some probability p, or randomly rotate left or right with probability 1-p. the rotation angle would be random from -r to r. Since this policy involves some randomness, it's called a *stochastic policy*. The robot will have an erratic trajectory, which guarantees it will eventually get to any place it can get and pick up all the dust. The question is how much dust it can picked up in 30 minutes?

How should you train this model? There are two *policy parameters* to tweak: the probability to move forward p and the angle range r. One possible is to try every possible combination, but this is not a good use of resources. An improved way is to try a bunch of random probability parameters and choose the combination that performs best. this is an example of *policy search*, in this case using a brute-force attack. When the *policy space* is too large (which is generally the case), finding a good set of parameters this way is like finding a needle in a big haystack.

![Four points in the policy space (left) and the agentâ€™s corresponding behavior (right)](image-1.png)

Another way is to use *genetic algorithm*. For example, you could randomly create a first generation of 100 policies and try them out, then "kill" the worst 80 policies (though it's maybe better to keep some of them, to persevere gene diversity) and make the 20 survivors produce 4 offsprings each. An offspring is a copy of its parent, plus some random variation. The surviving policies and their offsprings together run through the second run. if there is a single parent, this is called *asexual reproduction*. With two (or more) parents, it is called *sexual reproduction*. An offspring's genome (in this case is a set of policy parameter) is randomly composed of parts of its parents' genomes. You can continue to iterate though generations this way until you find a good enough policy. One interesting example of genetic algorithm sued for reinforcement learning is the [NeuroEvolution of Augmenting Topologies](https://neat-python.readthedocs.io/en/latest/neat_overview.html) (NEAT) algorithm. You can also check out this [video](), which implements and teaches a lot about this approach (this is my personal recommendation).

Yet another approach is to use optimization techniques, by evaluating the gradients of the rewards with regard to the policy parameters, then tweaking these parameter by following these gradients toward higher rewards. This is called *gradient ascent*: It's just like gradient descent, but in the opposite direction: maximizing instead of minimizing. We will talk about this approach in more detail later in this chapter. Going back to the vacuum cleaner example, we could slightly increase p and evaluate whether doing so increases the amount of dust picked up by the robot in 30 minutes; if it does, then increase p some more; if it doesn't, decrease it instead. We'll implement a popular PG (*policy gradient*) using TensorFlow, but first we need to create an environment for the agent ot live in - so it's time to introduce OpenAI Gymnasium.

# Introduction to OpenAI Gymnasium

One of the challenge of reinforcement learning is that in order to train an agent, you need a working environment. If you want a program that can learn to play an Atari game, you will need an Atari stimulator. If you want to program a walking robot, then the environment is the real world, and you can directly train your robot in your environment. However, this has some big limitations: If your robot fall of  a cliff, you can click Undo. You can't speed it up neither, as adding more computation pawer won't make the robot move any faster, and training 1,000 robots in parallel is way too expensive. In short, training is hard and expensive in real world, so you need a  *stimulated environment* at least for bootstrap training. For example, you might use [PyBullet]() or [MuJoCo]() for 3D physics stimulation.

OpenAI Gym is a toolkit that provides a wide variety of simulated environments (Atari games, board games, 2D and 3D physical stimulations, and so on), that you can use to train agents, compare them, or develop new RL algorithm. However, Gym has been moved to Gymnasium by the Farama Foundation, which luckily has backward compatibility with Gym, so we can download it instead and use `import gymnasium as gym` instead of `import gym`.

You can download Gymnasium using these commands:
```bash
%pip install -p -U gymnasium
%pip install -p -U gymnasium[classic_control, box2d, atari, accept-rom-license]
```
The `%` is only required in the notebook cell, you should remove it if running in terminal. The `-q` for *quiet*: it makes the output less verbose. The `-U` option stands for *upgrade*. The second command installs the library required to run various kind of environments. this includes classic environments from *control theory* - the science of controlling dynamical systems - such as balancing a pole on a cart. it also includes environment based on the Box2D library - a 2D physic engine for games. [You need to install `swig` beforehand](https://github.com/Farama-Foundation/Gymnasium/issues/662) however if you want to install Box2D, which can be done using `sudo apt-get install swig` in Linux. Lastly, it includes environments based on the Arcade Learning Environment (ALE), which is an emulator for Atari 2600 games. Several Atari game ROMs are downloaded automatically, and by running this code, you agree with Atari's ROM licenses. You can just use `gymnasium[all]` if you want all the dependencies. 

In the learning, I created a CartPole environment. This is a 2D stimulation in which a cart can be accelerate left or right in order to balance a pole placed on top of it. This is a classic control task.

The `gym.envs.registry` dictionary contains the names and specifications of all the variable environments.
After the environment is created, you must initialize it using the `reset()` method, optionally passing a random seed. This return the first observation. Observations depend on the type of environment. For the CartPole environment, each observation is a 1D NumPy array containing four floats representing the cart's horizontal position (`0.0` = center), its velocity (positive means right), the angle of the pole (`0.0` = vertical), and its angular velocity (positive means clockwise). The `reset()` method also returns a dictionary that may contain extra environment-specific information. This can be useful for debugging or training. For example, in many Atari environments, it contains the number of lives left. However, in the CartPole environment, this dictionary is empty.

Let's call the `render()` method to render this environment as an image. Since we set `sender_mode="rbg_array"` when creating this environment, the image will be returned as a NumPy array. You can then use Matplotlib's `imshow()` function to display this image, as usual.

You can use the `action_space` attribute to see what actions are possible. `Discrete(2)` means that the possible actions are integers 0 and 1, which representing accelerating left or right. Other environments may have additional discrete actions, or other kinds of actions (e.g., continuous).

The `step()` method executes the desired action and return five values:
- `obs`: This is the new observation. The cart is now moving toward the right (`obs[1] > 1`). The pole is still tilted toward the right (`obs[2] > 0`), but its angular velocity is now negative (`obs[3] < 0`), so will be likely to be titled toward the left in the next step.
- `reward`: In this environment, you get a reward of 1.0 at every step, no matter what you do, so the goal is to keep the episode running as long as possible.
- `done`: This value will be `True` when the episode is over. This will happen when the pole tilts too much, or goes off the screen, or after 200 steps (in this last case, you have won). After that, the environment must be reset before it can be used again.
- `truncated`: This value will be `True` when the episode is interrupted early, for example by an environment wrapper that imposes the maximum number of steps per episode (see [Gymnasium's documentation](https://gymnasium.farama.org/api/wrappers/) for more details on environment wrappers). Some RL algorithms treat truncated episodes differently from episodes finished normally (i.e., when `done` is `True`), but in this chapter we will treat them identically.
- `info`: This environment-specific dictionary may provide extra information, just like the one returned by the `reset()` method.
Once you have finished using an environment, you should call its `close()` method to free resources.

## A simple hardcoded policy

Let's hardcode a simple policy that accelerates left when the pole leaning toward the left and accelerates right when the pole leaning toward the right. We'll run this policy for 500 episodes and see its statistics. If you look at the result, even after 500 attempts, this policy never managed to keep the pole upright for more than 63 consecutive steps. Not good. If you look at the stimulation in this chapter's notebook, you'll see that the cart oscillates left and right more and more, until the pole tilts too much.

# Neural Network Policies

We can create a neural network policy. This neural network must take an observation as input, and output the action to be executed. In this case, this means the neural must have 4 neurons in the inputs layer, and only one in the output layer. More precisely, it will output an estimated probability for each action, and we will choose an action randomly based on these estimated probabilities. In the case of the CartPole environment, these are jst two possible actions, so we only need one output neuron. It will output the probability p of action 0 (left), and probability 1 - p of action 1 (right). For example, if the neuron output 0.7, then we choose action 0 with 70% probability, or action 1 with 30% probability.

![Neural network policy](image-2.png)

You may wonder why we are picking a random action based on the probabilities given by the neural network, rather than just picking the action with the highest score? This allows the model to find a good balance between *exploring* new actions and *exploiting* actions that known to be good. Here's an analogy: It's the first time you come to a new restaurant, and all the dished look equally good so you just pick a dish randomly. If it turns out to be great, you increase the chance to order it the next time, but never up to 100%, or else you may never try out other dishes, some of which may be even better than the one you tried. This *exploration/exploitation dilemma* is at the heart of reinforcement learning.

Also note that in this particular environment, the past actions and observations can be safely ignored, as each observation contains the full state of the environment. If there were some hidden states, then you might want to consider past actions and observations as well. For example, if we don't know the cart's velocity, but only the cart's position, then we need to know the previous position on order to estimate the current velocity. Another example is when the observations is noisy, hence we will need to know some past observations to estimate the most likely current state. TheCartPole is as simple as possible: The observations are noise-free, and they contain the environment's full states.

We can build a neural network using the `Sequential` API. You can see the implementation in the learning notebook. Because we just initialize it randomly, the neural network performed very badly.

# Evaluating Actions: The Credit Assignment Problem

We can create an architecture for the neural network, but how to train it? If we know the best action at each step, we could train the neural network as usual, by minimizing the cross entry between the estimate probability distribution and the target probability distribution. It would be just a regular supervised learning. However in reinforcement learning, the only guidance the agent gets is through reward, and rewards are typically sparse and delayed. For example, if the agent manages to balance the cart for 100 steps, how can it know which of the 100 actions were good, and which were bad? All it knows is the pole fell after the last action, but that last action is not fully responsible for the falling. This is called the *credit assignment problem*: when the agent gets a reward, it's hard for it to know which actions should get credited (or blamed) for it. Think of a dog getting a reward hours after it behaved well, will it understand that what it is rewarded for?

To tackle this problem, a common strategy is to evaluate an action based on the sum of all the rewards that comes after it, usually applying a *discount factor*, $\gamma$, at each step. This sum of discounted rewards is called the actions's *return*. If an agent decides to go right three times in a row and gets +10 reward after the first step, 0 after the second step, and finally -50 after the third step, then assuming we use the discount factor of $\gamma = 0.8$, the first action will have a return of $10 + \gamma \times 0 + \gamma^2 \times (-50) = -22$. If the discount factor is close to 0, then future rewards won't count too much compared to immediate rewards. Conversely, if the discount factor close to 1, then rewards far into the future will count almost as much as immediate rewards. Typical discount factors range from 0.9 to 0.99. With a discount factor of 0.95, rewards 13 into the future count roughly for half as much as immediate rewards (since $0.95^13 \approx 0.5$), while with a discount factor of 0.99, reward 69 steps into the future count for half as much as immediate rewards. In the CartPole environment, actions have fairly short-term effects, so choosing a discount of 0.95 seems reasonable.

![Computing an actionâ€™s return: the sum of discounted future rewards](image-3.png)

Of course, a good action may be followed by several of bad actions that cause the pole to fall quickly, which results in a good action getting a low return, makes the model to think that action is bad. Similarly, a good actor may sometimes star in a terrible movie. However, if we play the game enough times, on average good actions will get a higher return than bad ones. We want to estimate how much better or worse an action is, compared to the other possible actions, on average. This is called the *action advantage*. For this, we must run many episodes and normalize all the return values. After that, we can reasonably assume that actions with a negative advantage were bad while actions with positive advantage were good. Ok, now that we know how to evaluate each action, we're ready to train our first agent using policy gradients.

# Policy Gradient

As discussed earlier, PG algorithms optimize the parameters of a policy by following the gradients toward maximizing reward. One popular class of PG algorithms, called *REINFORCE algorithms*, was [introduced back in 1992](https://people.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf) by Ronald Williams. Here is a common variant:
- First, let the neural network policy play the game several times, and at every step, compute the gradients that would make the chosen action even more likely - but don't apply these gradients yet.
- Once you have run several episodes, compute each action's advantage using the method described in the last part.
- If an action's advantage is positive, it means that the action was probably good, and you want to apply the gradients computed earlier to make the action even more likely to be chosen in the future. However, if the action's advantage is negative, it means the action was probably bad, and you may want to apply the opposite gradients ot make this action less likely in the future. The solution is multiply each gradient vector by the corresponding action's advantage.
- Finally, compute the mean of all the resulting gradient vectors, and use it to perform a gradient descent step.

We will build from the ground up. First, we define a function to run the model for one step. We will pretend whatever actions it takes is the right one so that we can compute the loss and its gradients. These gradients will just be saved for a while, and we will modify them later depending on how good or bad the action turned out to be. We'll walk through the implementation in the learning notebook:
- Within the `GradientTape` block (see chapter 12), we start by giving the model observations. Since the model excepts a batch, we add one more axis in the outermost dimension to create a batch contains only one instance. This outputs the probability of going left.
- Next, we sample a random float between 0 and 1, and we check whether it is greater than `left_proba`. The action will be `False` with probability `left_proba`, or `True` with probability `1 - left_proba`. Once we cast this Boolean to an integer, the action will be 0 (left) or 1 (right).
- We now define the target probability of going left. As talked above, we pretend this action is the desired action, so it is 1 minus the action (cast to a float). If the action is 0 (left), then the target probability of going left will be 1. If the action is 1 (right), then the target probability of going left will be 0.
- Then we compute the loss using the given loss function, and we use the tape to compute the gradient fo the loss with regard to the model's trainable variables. You can think of these gradients as the way to increase the chance of doing this action with the model's policy. You don't know how good or bad this action will be, but follow these gradients will increase the chance of doing this action. These gradients will be tweaked later, depending on how good or bad the action turned out to be.
- Finally, we play the selected action, and we return the observations, reward, whether or not the episode is done or not, whether or not it is truncated or not, and the gradients we just computed.

We'll crate another function that will rely on the `play_one_step()` function to play multiple episodes, returning all the rewards and gradients for each episode and each step. The function will return a list of reward lists: one reward list per episode, containing one reward per step. It also returns a list of gradient lists: one gradient list per episode, each containing one tuple of gradients per step, and each tuple containing one gradient tensor per trainable variable.

The algorithm will use the `play_multiple_episodes()` function to play the game several times (e.g., 10 times), then it will go back and look at all the rewards, discount them and normalize them. To do that, we need a couple more functions; the first will compute the sum of future discounted rewards at each step, and the second will normalize all these discounted rewards (i.e., the returns) across all episodes by subtracting the mean and dividing by the standard deviation.

If you look at some test with the `discount_reward()` function, it returns exactly what we computed. You can also verify that the function `discount_and_normalize_reward()` function does normalize the action advantages in both episodes. Notice that the first action is much worse than the second, so its normalized advantage are all negative, while the second's are all positive. This means all the actions from the first episode are considered bad, while all from the second are considered good.

We are almost ready to run the algorithm. We just need to define some more hyperparameters: We will run 150 training iterations, run 10 episodes per iterations, and the maximum number of steps in each episode is 200. We'll use a discount factor of 0.95.

We also need an optimizer and a loss function. A Nadam optimizer with learning rate equals to 0.01 is fine, and since this is a binary classification task (there are two possible actions - left or right), we'll use binary cross-entropy loss function. Note that we pass the function itself, not an instance of it.

Now we can build the training loop:
- At each training iteration, we calculate all the rewards and gradients from every step of 10 episodes.
- We use `discount_and_normalize_rewards()` function to normalize all the actions advantage. This provides a hindsight of how actually these are actually good or bad.
- Next, we go through each trainable variable, for each of them, we computed the weighted mean of all the gradients times the normalized action associated to it, across every step in every episode.
- Finally, we use the optimizer to apply these mean gradient: the model's parameters will be tweaked in a way that will maximizing the action's advantage in all steps, thus hopefully the policy will be a bit better.

This code will train the neural network policy, and it will successfully learn to balance the pole on the cart. The mean reward per episode will be very close to 200, which means it's nearly won the game!

The simple policy gradients algorithm we just trained solved CartPole task, but it would not scale well to larger and more complex tasks. Indeed, it's highly *sample inefficient*, meaning it needs to explore the game for a very long time before it can make significant progress. This is due to the fact that it must run multiple episodes to estimate the advantage of each action, as we have seen. However, its's hte foundation of more powerful algorithms, such as *actor-critic* algorithms, which we will discuss briefly at end of this chapter.

Researchers try to find algorithms that work well even when the agent initially knows nothing about the environment. However, unless you're writing a paper, you should inject prior knowledge into the agent, as it would speed up training dramatically. For example, you know that the pole should be as vertical as possible, so you could add negative rewards proportional to the pole's angle. This will make rewards less parse and speed up training. Also, if you have already know a reasonably good policy (e.g., hardcoded), you may want to train the neural network to imitate it before using policy gradients to improve it.

# Markov Decision Processes

Whereas PG directly try to optimize the policy ot increase rewards, the algorithms we'll explore are less direct: the agent learns to estimate the expected return of each state or for each action in each state, then it uses this knowledge to decide how to act. To understand this class of algorithms, we must need to know *Markov decision processes* (MDPs). If you prefer an explanation using video, here is [my suggestion](https://www.youtube.com/watch?v=JGQe4kiPnrU).

In the early 20th century, the mathematician Andrey Markov studied stochastic processes with no memory, called *Markov chains*. Such a process had a fixed number of states,a nd it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to state s' is fixed, and it depends only on the pair (s, s'), not on past states.

![Example of a Markov chain](image-4.png)

Suppose the process start in state $s_0$, and there is a 70% chance it will remain in this state at the next step. Eventually it is bound to leave that state and never come back, since no state points back to state $s_0$. If it goes to state $s_1$, it most likely goes to state $s_2$ (with 90% chance), and immediately back to $s_1$ (with 100% chance). It may oscillate between these two states a number of times, but eventually it will go to state $s_3$ and remain there forever, since there's no way out: this is called a *terminal state*. Markov chains can have very different dynamics, and they are heavily used in thermodynamics, chemistry, statistics, and more.