1. 
- When the number of features is large, the Gradient Descent is the good choice (all 3 of Batch GD, SGD, Mini-batch GD is all good), just notice if the memory is enough if you choose Batch GD. 
- We shouldn't choose Normal equation and Singular Value Decomposition, as they don't scale well with the number of features. For example, if the number of features is n, then the time to compute the optimal value is $O(n^{2.4})$ to $O(n^3)$.
2. 
- Suppose my training data has ver different scales. Then all of the GD will be suffer from it, note that Normal equation and SVD would b fine.
- The way GD suffer from it is the following: Suppose the $x_1$ axis has a much bigger scale than the $x_2$ axis, then the GD will move along the $x_2$ axis first, but the problem is when it reaches near the optimal value in term of $x_2$ coordinate, it moves in the $x_1$ axis significantly slower (the significance proportional to the ratio of the scale of $x_1$ and $x_2$).  
- One more problem is that you can stuck with an suboptimal solution: As regularization penalizes large weights, features with smaller scale tend to ignore more compared to features with bigger scale.
- The solution is easy to find: Just scale the features before you train.
3. 
- It varies a lot on the chosen model and cost function.
- In our case, no, a GD model can't get stuck in a local minimum when training a logistic regression model with the MSE. 
- Because the cost function this time is a convex function, which means there is at most one suboptimal, which is the optimal value itself.
- However, note that if a specific different cost function used on a specific different model, the cost function could be not convex.
4. 
- No, it varies on the used model and as well. 
- Suppose the optimization problem (for example, Linear Regression and Logistic Regression) is convex and you choose a really small a really small learning rate and wait for a really long time, then in some sense, yes, all 3 algorithms will all end up the optimal value and output very similar model.
- However, if the learning rate is high, then the SGD and mini-batch GD will not even converge to a value: they just jump around the optimal value. Then even if you wait a really long time, the 3 algorithms could still end up with 3 different models.
5. Suppose I use batch gradient descent and when I plot the validation error at every epoch, I see the error increases consistently. There are 2 possibilities:
- If the training error is also high, then the learning rate may be too high and the model is diverging. The solution is to reduce the learning rate.
- If the training error is low, then the model is overfitting. I can fix it by regularizing the model, or keep feeding the model more training data.
6. No, it is not a good idea to stop mini-batch Gradient Descent immediately when the validation error goes up. Here are some reasons:
- Due to its somewhat stochastic nature, even when the model is underfitting, the validation can increases a bit. If you stop training as soon as the validation error increases, then the model may not even makes it near the optimum, let alone reaches the optimum.
- It will be better if we have some threshold and a waiting constant, such that if the validation error is not smaller by an amount of the threshold for the waiting constant epoch, then only that we will stop training.
7. 
- Among the 3 discussed GD algorithms, SGD will reach the vicinity (i.e. the neighbor area) of the optimum value the fattest. The reason is GD is bigger when the number of instance is bigger, so SGD, trains only on one instances, will have the fattest training iteration speed.
- Batch GD is also the only one to really converge, in a mathematical sense.
- Having a good learning rate schedule is the way to make SGD and mini-batch GD converges as well, that is gradually decreasing the learning rate when the model reaches near the optimum value.
8. Suppose I am using polynomial regression and there is a large gap between the validation error and train error. Then the model is overfitting. There are some ways to solve this:
    - Regularizing the model
    - Decrease the degree of the model
    - Change the model
    - Feeding more training data.
9. When the training error and validation error is both high, then the model is underfitting, which means it could be suffer from high bias. You should decrease the regularization hyperparameter $\alpha$ to let the model have a higher degree of freedom, which helps it fit the training data better.
10. 1. You should have a bit of regularization so you should favor Ridge Regression over plain Linear Regression.
    2. If you think that some features is irrelevant, then it could be good idea using Lasso instead of Ridge Regression.
    3. However, you should prefer Elastic Net than Lasso, because Lasso sometimes can perform erratically if some features is highly correlated or the number of features is bigger than the number of instances.
11. Because an image can be both outdoor and indoor, daytime and nighttime (e.g. an image can be taken indoor, beside an open windows, which can be classified as both indoor and outdoor), so our classes is not mutually exclusive. Therefore, you can't use Softmax Regression. Two Logistic regression classifiers, one for outdoor/indoor, one for daytime/nighttime, would be more suitable.